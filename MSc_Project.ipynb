{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/globalit6679/MSc-Project/blob/main/MSc_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50_X3iiYvqQX"
      },
      "source": [
        "## JAAD 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKpFQxou0dIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d83ffdc-54aa-4bce-a04e-e93fb18e049b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5BkNVhFZVas"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Interface for the JAAD dataset:\n",
        "\n",
        "A. Rasouli, I. Kotseruba, and J. K. Tsotsos,“Are they going to cross?\n",
        "a benchmark dataset and baseline for pedestrian crosswalk behavior,” In Proc.\n",
        "ICCV Workshop, 2017, pp. 206–213.\n",
        "\n",
        "A. Rasouli, I. Kotseruba, and J. K. Tsotsos, “Agreeing to cross: How drivers\n",
        "and pedestrians communicate,” In Proc. Intelligent Vehicles Symposium (IV),\n",
        "2017, pp. 264–269.\n",
        "\n",
        "I. Kotseruba, A. Rasouli, and J. K. Tsotsos, “Joint attention in autonomous\n",
        " driving (jaad),” arXiv:1609.04741, 2016.\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2018 I. Kotseruba\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\n",
        "\"\"\"\n",
        "import sys\n",
        "import pickle\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from os.path import join, abspath, exists\n",
        "from os import listdir, makedirs\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "\n",
        "class JAAD(object):\n",
        "    def __init__(self, data_path='', regen_pkl=False):\n",
        "        \"\"\"\n",
        "        Constructor of the jaad class\n",
        "        :param data_path: Path to the folder of the dataset\n",
        "        :param regen_pkl: Whether to regenerate the database\n",
        "        \"\"\"\n",
        "        self._year = '2016'\n",
        "        self._name = 'JAAD'\n",
        "        self._regen_pkl = regen_pkl\n",
        "        self._image_ext = '.png'\n",
        "\n",
        "        # Paths\n",
        "        self._jaad_path = data_path if data_path else self._get_default_path()\n",
        "        assert exists(self._jaad_path), \\\n",
        "            'Jaad path does not exist: {}'.format(self._jaad_path)\n",
        "        self._data_split_ids_path = join(self._jaad_path, 'split_ids')\n",
        "        self._annotation_path = join(self._jaad_path, 'annotations')\n",
        "        # self._annotation_vehicle_path = join(self._jaad_path, 'annotations_vehicle')\n",
        "        # self._annotation_traffic_path = join(self._jaad_path, 'annotations_traffic')\n",
        "        # self._annotation_attributes_path = join(self._jaad_path, 'annotations_attributes')\n",
        "        # self._annotation_appearance_path = join(self._jaad_path, 'annotations_appearance')\n",
        "        self._clips_path = join(self._jaad_path, 'videos')\n",
        "        self._images_path = join(self._jaad_path, 'images')\n",
        "\n",
        "    # Path generators\n",
        "    @property\n",
        "    def cache_path(self):\n",
        "        \"\"\"\n",
        "        Generate a path to save cache files\n",
        "        :return: Cache file folder path\n",
        "        \"\"\"\n",
        "        cache_path = abspath(join(self._jaad_path, 'data_cache'))\n",
        "        if not exists(cache_path):\n",
        "            makedirs(cache_path)\n",
        "        return cache_path\n",
        "\n",
        "    def _get_default_path(self):\n",
        "        \"\"\"\n",
        "        Return the default path where jaad_raw files are expected to be placed.\n",
        "        :return: the default path to the dataset folder\n",
        "        \"\"\"\n",
        "        return '/content/'\n",
        "\n",
        "    def _get_video_ids_split(self, image_set, subset='default'):\n",
        "        \"\"\"\n",
        "        Returns a list of video ids for a given data split\n",
        "        :param image_set: Data split, train, test, val\n",
        "        :return: The list of video ids\n",
        "        \"\"\"\n",
        "        vid_ids = []\n",
        "        sets = [image_set] if image_set != 'all' else ['train', 'test', 'val']\n",
        "        for s in sets:\n",
        "            vid_id_file = join(self._data_split_ids_path, subset, s + '.txt')\n",
        "            with open(vid_id_file, 'rt') as fid:\n",
        "                vid_ids.extend([x.strip() for x in fid.readlines()])\n",
        "        return vid_ids\n",
        "\n",
        "    def _get_video_ids(self):\n",
        "        \"\"\"\n",
        "        Returns a list of all video ids\n",
        "        :return: The list of video ids\n",
        "        \"\"\"\n",
        "        return [vid.split('.')[0] for vid in listdir(self._annotation_path)]\n",
        "\n",
        "    def _get_image_path(self, vid, fid):\n",
        "        \"\"\"\n",
        "          Generates the image path given ids\n",
        "          :param vid: Video id\n",
        "          :param fid: Frame id\n",
        "          :return: Return the path to the given image\n",
        "          \"\"\"\n",
        "        return join(self._images_path, vid,\n",
        "                    '{:05d}.png'.format(fid))\n",
        "\n",
        "    # Visual helpers\n",
        "    def update_progress(self, progress):\n",
        "        \"\"\"\n",
        "         Creates a progress bar\n",
        "         :param progress: The progress thus far\n",
        "         \"\"\"\n",
        "        barLength = 20\n",
        "        status = \"\"\n",
        "        if isinstance(progress, int):\n",
        "            progress = float(progress)\n",
        "\n",
        "        block = int(round(barLength * progress))\n",
        "        text = \"\\r[{}] {:0.2f}% {}\".format(\"#\" * block + \"-\" * (barLength - block), progress * 100, status)\n",
        "        sys.stdout.write(text)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def _print_dict(self, dic):\n",
        "        \"\"\"\n",
        "         Prints a dictionary, one key-value pair per line\n",
        "         :param dic: Dictionary\n",
        "         \"\"\"\n",
        "        for k, v in dic.items():\n",
        "            print('%s: %s' % (str(k), str(v)))\n",
        "\n",
        "    # Image processing helpers\n",
        "    def _squarify(self, bbox, ratio, img_width):\n",
        "        \"\"\"\n",
        "        Changes is the ratio of bounding boxes to a fixed ratio\n",
        "        :param bbox: Bounding box\n",
        "        :param ratio: Ratio to be changed to\n",
        "        :param img_width: Image width\n",
        "        :return: Squarified boduning box\n",
        "        \"\"\"\n",
        "        width = abs(bbox[0] - bbox[2])\n",
        "        height = abs(bbox[1] - bbox[3])\n",
        "        width_change = height * ratio - width\n",
        "\n",
        "        bbox[0] = bbox[0] - width_change / 2\n",
        "        bbox[2] = bbox[2] + width_change / 2\n",
        "        if bbox[0] < 0:\n",
        "            bbox[0] = 0\n",
        "\n",
        "        # check whether the new bounding box goes beyond image boarders\n",
        "        # If this is the case, the bounding box is shifted back\n",
        "        if bbox[2] > img_width:\n",
        "            bbox[0] = bbox[0] - bbox[2] + img_width\n",
        "            bbox[2] = img_width\n",
        "        return bbox\n",
        "\n",
        "    def extract_and_save_images(self):\n",
        "        \"\"\"\n",
        "        Extract images from clips and save on drive\n",
        "        \"\"\"\n",
        "\n",
        "        videos = [f.split('.')[0] for f in sorted(listdir(self._clips_path))]\n",
        "\n",
        "        for vid in videos:\n",
        "            path_to_file = join(self._annotation_path, vid + '.xml')\n",
        "            print(vid)\n",
        "            tree = ET.parse(path_to_file)\n",
        "            num_frames = int(tree.find(\"./meta/task/size\").text)\n",
        "\n",
        "            video_clip_path = join(self._clips_path, vid + '.mp4')\n",
        "\n",
        "            save_images_path = join(self._images_path, vid)\n",
        "            if not exists(save_images_path):\n",
        "                makedirs(save_images_path)\n",
        "\n",
        "            vidcap = cv2.VideoCapture(video_clip_path)\n",
        "            success, image = vidcap.read()\n",
        "            frame_num = 0\n",
        "            img_count = 0\n",
        "            if not success:\n",
        "                print('Failed to open the video {}'.format(vid))\n",
        "            while success:\n",
        "                self.update_progress(img_count / num_frames)\n",
        "                img_count += 1\n",
        "                if not exists(join(save_images_path, \"{:05d}.png\").format(frame_num)):\n",
        "                    cv2.imwrite(join(save_images_path, \"{:05d}.png\").format(frame_num), image)\n",
        "                else:\n",
        "                    print('path %s already exists')\n",
        "                success, image = vidcap.read()\n",
        "                frame_num += 1\n",
        "            if num_frames != img_count:\n",
        "                print('num images don\\'t match {}/{}'.format(num_frames, img_count))\n",
        "            print('\\n')\n",
        "\n",
        "    # Annotation processing helpers\n",
        "    def _map_text_to_scalar(self, label_type, value):\n",
        "        \"\"\"\n",
        "        Maps a text label in XML file to scalars\n",
        "        :param label_type: The label type\n",
        "        :param value: The text to be mapped\n",
        "        :return: The scalar value\n",
        "        \"\"\"\n",
        "        map_dic = {'occlusion': {'none': 0, 'part': 1, 'full': 2},\n",
        "                   'action': {'standing': 0, 'walking': 1},\n",
        "                   'nod': {'__undefined__': 0, 'nodding': 1},\n",
        "                   'look': {'not-looking': 0, 'looking': 1},\n",
        "                   'hand_gesture': {'__undefined__': 0, 'greet': 1, 'yield': 2,\n",
        "                                    'rightofway': 3, 'other': 4},\n",
        "                   'reaction': {'__undefined__': 0, 'clear_path': 1, 'speed_up': 2,\n",
        "                                'slow_down': 3},\n",
        "                   'cross': {'not-crossing': 0, 'crossing': 1, 'irrelevant': -1},\n",
        "                   'age': {'child': 0, 'young': 1, 'adult': 2, 'senior': 3},\n",
        "                   'designated': {'ND': 0, 'D': 1},\n",
        "                   'gender': {'n/a': 0, 'female': 1, 'male': 2},\n",
        "                   'intersection': {'no': 0, 'yes': 1},\n",
        "                   'motion_direction': {'n/a': 0, 'LAT': 1, 'LONG': 2},\n",
        "                   'traffic_direction': {'OW': 0, 'TW': 1},\n",
        "                   'signalized': {'n/a': 0, 'NS': 1, 'S': 2},\n",
        "                   'vehicle': {'stopped': 0, 'moving_slow': 1, 'moving_fast': 2,\n",
        "                               'decelerating': 3, 'accelerating': 4},\n",
        "                   'road_type': {'street': 0, 'parking_lot': 1, 'garage': 2},\n",
        "                   'traffic_light': {'n/a': 0, 'red': 1, 'green': 2}}\n",
        "\n",
        "        return map_dic[label_type][value]\n",
        "\n",
        "    def _map_scalar_to_text(self, label_type, value):\n",
        "        \"\"\"\n",
        "        Maps a scalar value to a text label\n",
        "        :param label_type: The label type\n",
        "        :param value: The scalar to be mapped\n",
        "        :return: The text label\n",
        "        \"\"\"\n",
        "        map_dic = {'occlusion': {0: 'none', 1: 'part', 2: 'full'},\n",
        "                   'action': {0: 'standing', 1: 'walking'},\n",
        "                   'nod': {0: '__undefined__', 1: 'nodding'},\n",
        "                   'look': {0: 'not-looking', 1: 'looking'},\n",
        "                   'hand_gesture': {0: '__undefined__', 1: 'greet',\n",
        "                                    2: 'yield', 3: 'rightofway',\n",
        "                                    4: 'other'},\n",
        "                   'reaction': {0: '__undefined__', 1: 'clear_path',\n",
        "                                2: 'speed_up', 3: 'slow_down'},\n",
        "                   'cross': {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'},\n",
        "                   'age': {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'},\n",
        "                   'designated': {0: 'ND', 1: 'D'},\n",
        "                   'gender': {0: 'n/a', 1: 'female', 2: 'male'},\n",
        "                   'intersection': {0: 'no', 1: 'yes'},\n",
        "                   'motion_direction': {0: 'n/a', 1: 'LAT', 2: 'LONG'},\n",
        "                   'traffic_direction': {0: 'OW', 1: 'TW'},\n",
        "                   'signalized': {0: 'n/a', 1: 'NS', 2: 'S'},\n",
        "                   'vehicle': {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast',\n",
        "                               3: 'decelerating', 4: 'accelerating'},\n",
        "                   'road_type': {0: 'street', 1: 'parking_lot', 2: 'garage'},\n",
        "                   'traffic_light': {0: 'n/a', 1: 'red', 2: 'green'}}\n",
        "\n",
        "        return map_dic[label_type][value]\n",
        "\n",
        "    def _get_annotations(self, vid):\n",
        "        \"\"\"\n",
        "        Generates a dictinary of annotations by parsing the video XML file\n",
        "        :param vid: The id of video to parse\n",
        "        :return: A dictionary of annotations\n",
        "        \"\"\"\n",
        "        path_to_file = join(self._annotation_path, vid + '.xml')\n",
        "        tree = ET.parse(path_to_file)\n",
        "        ped_annt = 'ped_annotations'\n",
        "\n",
        "        annotations = {}\n",
        "        annotations['num_frames'] = int(tree.find(\"./meta/task/size\").text)\n",
        "        annotations['width'] = int(tree.find(\"./meta/task/original_size/width\").text)\n",
        "        annotations['height'] = int(tree.find(\"./meta/task/original_size/height\").text)\n",
        "        annotations[ped_annt] = {}\n",
        "\n",
        "        ped_tracks = tree.findall(\"./track\")\n",
        "\n",
        "        for t in ped_tracks:\n",
        "            boxes = t.findall('./box')\n",
        "            new_id = boxes[0].find('./attribute[@name=\\\"id\\\"]').text\n",
        "            old_id = boxes[0].find('./attribute[@name=\\\"old_id\\\"]').text\n",
        "            annotations[ped_annt][new_id] = {'old_id': old_id, 'frames': [],\n",
        "                                             'bbox': [], 'occlusion': []}\n",
        "            if 'pedestrian' in old_id:\n",
        "                annotations['ped_annotations'][new_id]['behavior'] = {'cross': [],\n",
        "                                                                      'reaction': [],\n",
        "                                                                      'hand_gesture': [],\n",
        "                                                                      'look': [],\n",
        "                                                                      'action': [],\n",
        "                                                                      'nod': []}\n",
        "            else:\n",
        "                annotations[ped_annt][new_id]['behavior'] = {}\n",
        "\n",
        "            for b in boxes:\n",
        "                annotations[ped_annt][new_id]['bbox'].append(\n",
        "                    [float(b.get('xtl')), float(b.get('ytl')),\n",
        "                     float(b.get('xbr')), float(b.get('ybr'))])\n",
        "                occ = self._map_text_to_scalar('occlusion',\n",
        "                                               b.find('./attribute[@name=\\\"occlusion\\\"]').text)\n",
        "                annotations[ped_annt][new_id]['occlusion'].append(occ)\n",
        "                annotations[ped_annt][new_id]['frames'].append(int(b.get('frame')))\n",
        "                for beh in annotations['ped_annotations'][new_id]['behavior'].keys():\n",
        "                    annotations[ped_annt][new_id]['behavior'][beh].append(\n",
        "                        self._map_text_to_scalar(beh,\n",
        "                                                 b.find('./attribute[@name=\\\"' + beh + '\\\"]').text))\n",
        "\n",
        "        return annotations\n",
        "\n",
        "    def _get_ped_attributes(self, vid):\n",
        "        \"\"\"\n",
        "        Generates a dictinary of attributes by parsing the video XML file\n",
        "        :param vid: The id of video to parse\n",
        "        :return: A dictionary of attributes\n",
        "        \"\"\"\n",
        "        path_to_file = join(self._annotation_attributes_path, vid + '_attributes.xml')\n",
        "        tree = ET.parse(path_to_file)\n",
        "\n",
        "        attributes = {}\n",
        "        pedestrians = tree.findall(\"./pedestrian\")\n",
        "        for p in pedestrians:\n",
        "            new_id = p.get('id')\n",
        "            old_id = p.get('old_id')\n",
        "            attributes[new_id] = {'old_id': old_id}\n",
        "            for k, v in p.items():\n",
        "                if 'id' in k:\n",
        "                    continue\n",
        "                try:\n",
        "                    attributes[new_id][k] = int(v)\n",
        "                except ValueError:\n",
        "                    attributes[new_id][k] = self._map_text_to_scalar(k, v)\n",
        "\n",
        "        return attributes\n",
        "\n",
        "    def _get_ped_appearance(self, vid):\n",
        "        \"\"\"\n",
        "        Generates a dictinary of appearance annotations by parsing the video XML file\n",
        "        :param vid: The id of video to parse. The labels are as follows:\n",
        "            - pose_front, pose_back... - coarse pose of the pedestrian relative to the camera\n",
        "            - clothes_below_knee - long clothing\n",
        "            - clothes_upper_light, clothes_lower_dark... - coarse clothing color above/below waist\n",
        "            - backpack - presence of a backpack (worn on the back, not held in hand)\n",
        "            - bag_hand, bag_elbow, bag_shoulder - whether bag(s) are held in a hand, on a bent elbow or worn on a shoulder\n",
        "            - bag_left_side, bag_right_side - whether bag(s) appear on the left/right side of the pedestrian body\n",
        "            - cap,hood - headwear\n",
        "            - umbrella,phone,baby,object - various things carried by the pedestrians\n",
        "            - stroller/cart - objects being pushed by the pedestrian\n",
        "            - bicycle/motorcycle - for pedestrians riding or walking these vehicles\n",
        "        :return: A dictionary of appearance annotations\n",
        "        \"\"\"\n",
        "        labels = ['pose_front', 'pose_back', 'pose_left', 'pose_right',\n",
        "                  'clothes_below_knee', 'clothes_upper_light', 'clothes_upper_dark', 'clothes_lower_light',\n",
        "                  'clothes_lower_dark', 'backpack', 'bag_hand', 'bag_elbow',\n",
        "                  'bag_shoulder', 'bag_left_side', 'bag_right_side', 'cap',\n",
        "                  'hood', 'sunglasses', 'umbrella', 'phone',\n",
        "                  'baby', 'object', 'stroller_cart', 'bicycle_motorcycle']\n",
        "        path_to_file = join(self._annotation_appearance_path , vid + '_appearance.xml')\n",
        "        tree = ET.parse(path_to_file)\n",
        "        annotations = {}\n",
        "        ped_tracks = tree.findall(\"./track\")\n",
        "        for t in ped_tracks:\n",
        "            boxes = t.findall('./box')\n",
        "            new_id = t.get(\"id\")\n",
        "            annotations[new_id] = dict(zip(labels, [[] for _ in range(len(labels))]))\n",
        "            annotations[new_id]['frames'] = []\n",
        "            for b in boxes:\n",
        "                annotations[new_id]['frames'].append(int(b.get('frame')))\n",
        "                for l in labels:\n",
        "                    annotations[new_id][l].append(b.get(l))\n",
        "        return annotations\n",
        "\n",
        "    def _get_traffic_attributes(self, vid):\n",
        "        \"\"\"\n",
        "        Generates a dictinary of vehicle attributes by parsing the video XML file\n",
        "        :param vid: The id of video to parse\n",
        "        :return: A dictionary of vehicle attributes\n",
        "        \"\"\"\n",
        "        path_to_file = join(self._annotation_traffic_path, vid + '_traffic.xml')\n",
        "        tree = ET.parse(path_to_file)\n",
        "        road_type = tree.find(\"./road_type\").text\n",
        "        traffic_attributes = {'road_type': self._map_text_to_scalar('road_type', road_type)}\n",
        "        frames = tree.findall(\"./frame\")\n",
        "        for f in frames:\n",
        "            traffic_attributes[int(f.get('id'))] = {'ped_crossing': f.get('ped_crossing'),\n",
        "                                                    'ped_sign': f.get('ped_sign'),\n",
        "                                                    'stop_sign': f.get('stop_sign'),\n",
        "                                                    'traffic_light': self._map_text_to_scalar('traffic_light',\n",
        "                                                                                             f.get('traffic_light'))}\n",
        "\n",
        "        return traffic_attributes\n",
        "\n",
        "    def _get_vehicle_attributes(self, vid):\n",
        "        \"\"\"\n",
        "        Generates a dictinary of vehicle attributes by parsing the video XML file\n",
        "        :param vid: The id of video to parse\n",
        "        :return: A dictionary of vehicle attributes\n",
        "        \"\"\"\n",
        "        path_to_file = join(self._annotation_vehicle_path, vid + '_vehicle.xml')\n",
        "        tree = ET.parse(path_to_file)\n",
        "\n",
        "        veh_attributes = {}\n",
        "        frames = tree.findall(\"./frame\")\n",
        "        for f in frames:\n",
        "            veh_attributes[int(f.get('id'))] = self._map_text_to_scalar('vehicle', f.get('action'))\n",
        "\n",
        "        return veh_attributes\n",
        "\n",
        "    def generate_database(self):\n",
        "        \"\"\"\n",
        "        Generate a database of jaad dataset by integrating all annotations\n",
        "        Dictionary structure:\n",
        "        'vid_id'(str): {\n",
        "            'num_frames': int\n",
        "            'width': int\n",
        "            'height': int\n",
        "            'ped_annotations'(str): {\n",
        "                'ped_id'(str): {\n",
        "                    'old_id': str\n",
        "                    'frames: list(int)\n",
        "                    'occlusion': list(int)\n",
        "                    'bbox': list([x1, y1, x2, y2])\n",
        "                    'behavior'(str): {\n",
        "                        'action': list(int)\n",
        "                        'reaction': list(int)\n",
        "                        'nod': list(int)\n",
        "                        'hand_gesture': list(int)\n",
        "                        'cross': list(int)\n",
        "                        'look': list(int)\n",
        "                    'appearance'(str): {\n",
        "                        'pose_front':list(int)\n",
        "                        'pose_back':list(int)\n",
        "                        'pose_left':list(int)\n",
        "                        'pose_right':list(int)\n",
        "                        'clothes_below_knee':list(int)\n",
        "                        'clothes_upper_light':list(int)\n",
        "                        'clothes_upper_dark':list(int)\n",
        "                        'clothes_lower_light':list(int)\n",
        "                        'clothes_lower_dark':list(int)\n",
        "                        'backpack':list(int)\n",
        "                        'bag_hand':list(int)\n",
        "                        'bag_elbow':list(int)\n",
        "                        'bag_shoulder':list(int)\n",
        "                        'bag_left_side':list(int)\n",
        "                        'bag_right_side':list(int)\n",
        "                        'cap':list(int)\n",
        "                        'hood':list(int)\n",
        "                        'sunglasses':list(int)\n",
        "                        'umbrella':list(int)\n",
        "                        'phone':list(int)\n",
        "                        'baby':list(int)\n",
        "                        'object':list(int)\n",
        "                        'stroller_cart':list(int)\n",
        "                        'bicycle_motorcycle':list(int)\n",
        "                    'attributes'(str): {\n",
        "                         'age': int\n",
        "                         'old_id': str\n",
        "                         'num_lanes': int\n",
        "                         'crossing': int\n",
        "                         'gender': int\n",
        "                         'crossing_point': int\n",
        "                         'decision_point': int\n",
        "                         'intersection': int\n",
        "                         'designated': int\n",
        "                         'signalized': int\n",
        "                         'traffic_direction': int\n",
        "                         'group_size': int\n",
        "                         'motion_direction': int\n",
        "            'vehicle_annotations'(str): {\n",
        "                frames(int):{\n",
        "                    action: int\n",
        "            'traffic_annotations'(str): {\n",
        "                road_type: int\n",
        "                frames(int):{\n",
        "                    ped_crossing: int\n",
        "                    ped_sign: int\n",
        "                    stop_sign: int\n",
        "                    traffic_light: int\n",
        "\n",
        "        :return: A database dictionary\n",
        "        \"\"\"\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Generating database for jaad\")\n",
        "\n",
        "        # Generates a list of behavioral xml file names for  videos\n",
        "        cache_file = join(self.cache_path, 'jaad_database.pkl')\n",
        "        if exists(cache_file) and not self._regen_pkl:\n",
        "            with open(cache_file, 'rb') as fid:\n",
        "                try:\n",
        "                    database = pickle.load(fid)\n",
        "                except:\n",
        "                    database = pickle.load(fid, encoding='bytes')\n",
        "            print('jaad database loaded from {}'.format(cache_file))\n",
        "            return database\n",
        "\n",
        "        video_ids = sorted(self._get_video_ids())\n",
        "        database = {}\n",
        "        for vid in video_ids:\n",
        "            #print('Getting annotations for %s' % vid)\n",
        "            vid_annotations = self._get_annotations(vid)\n",
        "            vid_attributes = self._get_ped_attributes(vid)\n",
        "            vid_appearance = self._get_ped_appearance(vid)\n",
        "            vid_veh_annotations = self._get_vehicle_attributes(vid)\n",
        "            vid_traffic_annotations = self._get_traffic_attributes(vid)\n",
        "\n",
        "            # Combining all annotations\n",
        "            vid_annotations['vehicle_annotations'] = vid_veh_annotations\n",
        "            vid_annotations['traffic_annotations'] = vid_traffic_annotations\n",
        "            for ped in vid_annotations['ped_annotations']:\n",
        "                try:\n",
        "                    vid_annotations['ped_annotations'][ped]['attributes'] = vid_attributes[ped]\n",
        "                except KeyError:\n",
        "                    vid_annotations['ped_annotations'][ped]['attributes'] = {}\n",
        "                try:\n",
        "                    vid_annotations['ped_annotations'][ped]['appearance'] = vid_appearance[ped]\n",
        "                except KeyError:\n",
        "                    vid_annotations['ped_annotations'][ped]['appearance'] = {}\n",
        "\n",
        "            database[vid] = vid_annotations\n",
        "\n",
        "        with open(cache_file, 'wb') as fid:\n",
        "            pickle.dump(database, fid, pickle.HIGHEST_PROTOCOL)\n",
        "        print('The database is written to {}'.format(cache_file))\n",
        "\n",
        "        return database\n",
        "\n",
        "    def get_data_stats(self):\n",
        "        \"\"\"\n",
        "        Generates statistics for jaad dataset\n",
        "        \"\"\"\n",
        "        annotations = self.generate_database()\n",
        "\n",
        "        videos_count = len(annotations.keys())\n",
        "        ped_box_beh_count = 0\n",
        "        ped_beh_count = 0\n",
        "        ped_count = 0\n",
        "        ped_box_count = 0\n",
        "        people_count = 0\n",
        "        people_box_count = 0\n",
        "        total_frames = 0\n",
        "\n",
        "        for vid in annotations:\n",
        "            total_frames += annotations[vid]['num_frames']\n",
        "            for ped in annotations[vid]['ped_annotations']:\n",
        "\n",
        "                if 'b' in ped:\n",
        "                    ped_beh_count += 1\n",
        "                    ped_box_beh_count += len(annotations[vid]['ped_annotations'][ped]['bbox'])\n",
        "                elif 'p' in ped:\n",
        "                    people_count += 1\n",
        "                    people_box_count += len(annotations[vid]['ped_annotations'][ped]['bbox'])\n",
        "                else:\n",
        "                    ped_count += 1\n",
        "                    ped_box_count += len(annotations[vid]['ped_annotations'][ped]['bbox'])\n",
        "\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Number of videos: %d\" % videos_count)\n",
        "        print(\"Number of frames: %d\" % total_frames)\n",
        "        print(\"Number of pedestrians with behavior tag: %d\" % ped_beh_count)\n",
        "        print(\"Number of pedestrians with no behavior tag: %d\" % ped_count)\n",
        "        print(\"Number of people: %d\" % people_count)\n",
        "        print(\"Total number of pedestrians: %d\" % (ped_count + ped_beh_count + people_count))\n",
        "\n",
        "        print(\"Number of pedestrian bounding boxes with behavior tag: %d\" % ped_box_beh_count)\n",
        "        print(\"Number of pedestrian bounding boxes with no behavior tag: %d\" % ped_box_count)\n",
        "        print(\"Number of people bounding boxes: %d\" % people_box_count)\n",
        "        print(\"Total number of pedestrian bounding boxes: %d\" % (ped_box_beh_count + ped_box_count))\n",
        "\n",
        "    def balance_samples_count(self, seq_data, label_type, random_seed=42):\n",
        "        \"\"\"\n",
        "        Balances the number of positive and negative samples by randomly sampling\n",
        "        from the more represented samples. Only works for binary classes.\n",
        "        :param seq_data: The sequence data to be balanced.\n",
        "        :param label_type: The lable type based on which the balancing takes place.\n",
        "        The label values must be binary, i.e. only 0, 1.\n",
        "        :param random_seed: The seed for random number generator.\n",
        "        :return: Balanced data sequence.\n",
        "        \"\"\"\n",
        "        for lbl in seq_data[label_type]:\n",
        "            for i in lbl:\n",
        "                if i[0] not in [0, 1]:\n",
        "                    raise Exception(\"The label values used for balancing must be\"\n",
        "                                    \" either 0 or 1\")\n",
        "\n",
        "        # balances the number of positive and negative samples\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Balancing the number of positive and negative intention samples\")\n",
        "\n",
        "        gt_labels = [gt[0] for gt in seq_data[label_type]]\n",
        "        num_pos_samples = np.count_nonzero(np.array(gt_labels))\n",
        "        num_neg_samples = len(gt_labels) - num_pos_samples\n",
        "\n",
        "        new_seq_data = {}\n",
        "        # finds the indices of the samples with larger quantity\n",
        "        if num_neg_samples == num_pos_samples:\n",
        "            print('Positive and negative samples are already balanced')\n",
        "            return seq_data\n",
        "        else:\n",
        "            print('Unbalanced: \\t Positive: {} \\t Negative: {}'.format(num_pos_samples, num_neg_samples))\n",
        "            if num_neg_samples > num_pos_samples:\n",
        "                rm_index = np.where(np.array(gt_labels) == 0)[0]\n",
        "            else:\n",
        "                rm_index = np.where(np.array(gt_labels) == 1)[0]\n",
        "\n",
        "            # Calculate the difference of sample counts\n",
        "            dif_samples = abs(num_neg_samples - num_pos_samples)\n",
        "            # shuffle the indices\n",
        "            np.random.seed(random_seed)\n",
        "            np.random.shuffle(rm_index)\n",
        "            # reduce the number of indices to the difference\n",
        "            rm_index = rm_index[0:dif_samples]\n",
        "            # update the data\n",
        "            for k in seq_data:\n",
        "                seq_data_k = seq_data[k]\n",
        "                if not isinstance(seq_data[k], list):\n",
        "                    new_seq_data[k] = seq_data[k]\n",
        "                else:\n",
        "                    new_seq_data[k] = [seq_data_k[i] for i in range(0, len(seq_data_k)) if i not in rm_index]\n",
        "\n",
        "            new_gt_labels = [gt[0] for gt in new_seq_data[label_type]]\n",
        "            num_pos_samples = np.count_nonzero(np.array(new_gt_labels))\n",
        "            print('Balanced:\\t Positive: %d  \\t Negative: %d\\n'\n",
        "                  % (num_pos_samples, len(new_seq_data[label_type]) - num_pos_samples))\n",
        "        return new_seq_data\n",
        "\n",
        "    # Pedestrian id helpers\n",
        "    def _get_pedestrian_ids(self, sample_type='all'):\n",
        "        \"\"\"\n",
        "        Get all pedestrian ids\n",
        "        :return: A list of pedestrian ids\n",
        "        \"\"\"\n",
        "        annotations = self.generate_database()\n",
        "        pids = []\n",
        "        for vid in sorted(annotations):\n",
        "            if sample_type == 'beh':\n",
        "                pids.extend([p for p in annotations[vid]['ped_annotations'].keys() if 'b' in p])\n",
        "            else:\n",
        "                pids.extend(annotations[vid]['ped_annotations'].keys())\n",
        "        return pids\n",
        "\n",
        "    def _get_random_pedestrian_ids(self, image_set, ratios=None, val_data=True, regen_data=False, sample_type='all'):\n",
        "        \"\"\"\n",
        "        Generates and save a database of activities for all pedestriasns\n",
        "        :param image_set: The data split to return\n",
        "        :param ratios: The ratios to split the data. There should be 2 ratios (or 3 if val_data is true)\n",
        "        and they should sum to 1. e.g. [0.4, 0.6], [0.3, 0.5, 0.2]\n",
        "        :param val_data: Whether to generate validation data\n",
        "        :param regen_data: Whether to overwrite the existing data\n",
        "        :return: The random sample split\n",
        "        \"\"\"\n",
        "\n",
        "        assert image_set in ['train', 'test', 'val']\n",
        "        cache_file = join(self.cache_path, \"random_samples.pkl\")\n",
        "        if exists(cache_file) and not regen_data:\n",
        "            print(\"Random sample currently exists.\\n Loading from %s\" % cache_file)\n",
        "            with open(cache_file, 'rb') as fid:\n",
        "                try:\n",
        "                    rand_samples = pickle.load(fid)\n",
        "                except:\n",
        "                    rand_samples = pickle.load(fid, encoding='bytes')\n",
        "                assert image_set in rand_samples, \"%s does not exist in random samples\\n\" \\\n",
        "                                                  \"Please try again by setting regen_data = True\" % image_set\n",
        "                if val_data:\n",
        "                    assert len(rand_samples['ratios']) == 3, \"The existing random samples \" \\\n",
        "                                                             \"does not have validation data.\\n\" \\\n",
        "                                                             \"Please try again by setting regen_data = True\"\n",
        "                if ratios is not None:\n",
        "                    assert ratios == rand_samples['ratios'], \"Specified ratios {} does not match the ones in existing file {}.\\n\\\n",
        "                                                              Perform one of the following options:\\\n",
        "                                                              1- Set ratios to None\\\n",
        "                                                              2- Set ratios to the same values \\\n",
        "                                                              3- Regenerate data\".format(ratios, rand_samples['ratios'])\n",
        "\n",
        "                print('The ratios are {}'.format(rand_samples['ratios']))\n",
        "                print(\"Number of %s tracks %d\" % (image_set, len(rand_samples[image_set])))\n",
        "                return rand_samples[image_set]\n",
        "\n",
        "        if ratios is None:\n",
        "            if val_data:\n",
        "                ratios = [0.5, 0.4, 0.1]\n",
        "            else:\n",
        "                ratios = [0.5, 0.5]\n",
        "\n",
        "        assert sum(ratios) > 0.999999, \"Ratios {} do not sum to 1\".format(ratios)\n",
        "        if val_data:\n",
        "            assert len(ratios) == 3, \"To generate validation data three ratios should be selected\"\n",
        "        else:\n",
        "            assert len(ratios) == 2, \"With no validation only two ratios should be selected\"\n",
        "\n",
        "        print(\"################ Generating Random training/testing data ################\")\n",
        "        ped_ids = self._get_pedestrian_ids(sample_type)\n",
        "        print(\"Total number of tracks %d\" % len(ped_ids))\n",
        "        print('The ratios are {}'.format(ratios))\n",
        "        sample_split = {'ratios': ratios}\n",
        "        train_samples, test_samples = train_test_split(ped_ids, train_size=ratios[0])\n",
        "        print(\"Number of train tracks %d\" % len(train_samples))\n",
        "\n",
        "        if val_data:\n",
        "            test_samples, val_samples = train_test_split(test_samples, train_size=ratios[1] / sum(ratios[1:]))\n",
        "            print(\"Number of val tracks %d\" % len(val_samples))\n",
        "            sample_split['val'] = val_samples\n",
        "\n",
        "        print(\"Number of test tracks %d\" % len(test_samples))\n",
        "        sample_split['train'] = train_samples\n",
        "        sample_split['test'] = test_samples\n",
        "\n",
        "        cache_file = join(self.cache_path, \"random_samples.pkl\")\n",
        "        with open(cache_file, 'wb') as fid:\n",
        "            pickle.dump(sample_split, fid, pickle.HIGHEST_PROTOCOL)\n",
        "            print('jaad {} samples written to {}'.format('random', cache_file))\n",
        "        return sample_split[image_set]\n",
        "\n",
        "    def _get_kfold_pedestrian_ids(self, image_set, num_folds=5, fold=1, sample_type='all'):\n",
        "        \"\"\"\n",
        "        Generate kfold pedestrian ids\n",
        "        :param image_set: Image set split\n",
        "        :param num_folds: Number of folds\n",
        "        :param fold: The given fold\n",
        "        :return: List of pedestrian ids for the given fold\n",
        "        \"\"\"\n",
        "        assert image_set in ['train', 'test'], \"For K-fold data split, image-set should be either \\\"train\\\" or \\\"test\\\"\"\n",
        "        assert fold <= num_folds, \"Fold number should be smaller than number of folds\"\n",
        "        print(\"################ Generating %d fold data ################\" % num_folds)\n",
        "        cache_file = join(self.cache_path, \"%d_fold_samples.pkl\" % num_folds)\n",
        "\n",
        "        if exists(cache_file):\n",
        "            print(\"Loading %d-fold data from %s\" % (num_folds, cache_file))\n",
        "            with open(cache_file, 'rb') as fid:\n",
        "                try:\n",
        "                    fold_idx = pickle.load(fid)\n",
        "                except:\n",
        "                    fold_idx = pickle.load(fid, encoding='bytes')\n",
        "        else:\n",
        "            ped_ids = self._get_pedestrian_ids(sample_type)\n",
        "            kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "            fold_idx = {'pid': ped_ids}\n",
        "            count = 1\n",
        "            for train_index, test_index in kf.split(ped_ids):\n",
        "                fold_idx[count] = {'train': train_index.tolist(), 'test': test_index.tolist()}\n",
        "                count += 1\n",
        "            with open(cache_file, 'wb') as fid:\n",
        "                pickle.dump(fold_idx, fid, pickle.HIGHEST_PROTOCOL)\n",
        "                print('jaad {}-fold samples written to {}'.format(num_folds, cache_file))\n",
        "        print(\"Number of %s tracks %d\" % (image_set, len(fold_idx[fold][image_set])))\n",
        "        kfold_ids = [fold_idx['pid'][i] for i in range(len(fold_idx['pid'])) if i in fold_idx[fold][image_set]]\n",
        "        return kfold_ids\n",
        "\n",
        "    # Pedestrian detection generators\n",
        "    def get_detection_data(self, image_set, method, occlusion_type=None, file_path='data/', **params):\n",
        "        \"\"\"\n",
        "        Generates data for pedestrian detection algorithms\n",
        "        :param image_set: Split set name\n",
        "        :param method: Detection algorithm: frcnn, retinanet, yolo3, ssd\n",
        "        :param occlusion_type: The types of occlusion: None: only unoccluded samples\n",
        "                                                       part: Unoccluded and partially occluded samples\n",
        "                                                       full: All samples\n",
        "        :param file_path: Where to save the script file\n",
        "        :return: Pedestrian samples\n",
        "        \"\"\"\n",
        "        squarify_ratio = params['squarify_ratio']\n",
        "        frame_stride = params['fstride']\n",
        "        height_rng = params['height_rng']\n",
        "        if not exists(file_path):\n",
        "            makedirs(file_path)\n",
        "        if height_rng is None:\n",
        "            height_rng = [0, float('inf')]\n",
        "\n",
        "        annotations = self.generate_database()\n",
        "        video_ids, _pids = self._get_data_ids(image_set, params)\n",
        "\n",
        "        ped_samples = {}\n",
        "        unique_samples = []\n",
        "        total_sample_count = 0\n",
        "        for vid in video_ids:\n",
        "            img_width = annotations[vid]['width']\n",
        "            img_height = annotations[vid]['height']\n",
        "            num_frames = annotations[vid]['num_frames']\n",
        "            for i in range(0,num_frames,frame_stride):\n",
        "                ped_samples[join(self._jaad_path, 'images', vid, '{:05d}.png'.format(i))] = []\n",
        "            for pid in annotations[vid]['ped_annotations']:\n",
        "                if params['data_split_type'] != 'default' and pid not in _pids:\n",
        "                    continue\n",
        "                difficult =  0\n",
        "                if 'p' in pid:\n",
        "                    difficult = -1\n",
        "                    if image_set in ['train', 'val']:\n",
        "                        continue\n",
        "                imgs = [join(self._jaad_path, 'images', vid, '{:05d}.png'.format(f)) for f in \\\n",
        "                        annotations[vid]['ped_annotations'][pid]['frames']]\n",
        "                boxes = annotations[vid]['ped_annotations'][pid]['bbox']\n",
        "                occlusion = annotations[vid]['ped_annotations'][pid]['occlusion']\n",
        "                for i, b in enumerate(boxes):\n",
        "                    if imgs[i] not in ped_samples:\n",
        "                        continue\n",
        "                    bbox_height = abs(b[0] - b[2])\n",
        "                    if height_rng[0] <= bbox_height <= height_rng[1]:\n",
        "                        if (occlusion_type == None and occlusion[i] == 0) or \\\n",
        "                                (occlusion_type == 'part' and occlusion[i] < 2) or \\\n",
        "                                (occlusion_type == 'full'):\n",
        "                            if squarify_ratio:\n",
        "                                b = self._squarify(b, squarify_ratio, img_width)\n",
        "                            ped_samples[imgs[i]].append(\n",
        "                                                {'width': img_width,\n",
        "                                                'height': img_height,\n",
        "                                                'tag': pid,\n",
        "                                                'box': b,\n",
        "                                                'seg_area': (b[2] - b[0] + 1) * (b[3] - b[1] + 1),\n",
        "                                                'occlusion': occlusion[i],\n",
        "                                                'difficult': difficult})\n",
        "                            if pid not in unique_samples:\n",
        "                                unique_samples.append(pid)\n",
        "                            total_sample_count += 1\n",
        "        print('Number of unique pedestrians %d ' % len(unique_samples))\n",
        "        print('Number of samples %d ' % total_sample_count)\n",
        "        if method == 'frcnn':\n",
        "            return self._get_data_frcnn(ped_samples)\n",
        "        elif method == 'retinanet':\n",
        "            return self._generate_csv_data_retinanet(image_set, file_path, ped_samples)\n",
        "        elif method == 'yolo3':\n",
        "            return self._generate_csv_data_yolo3(image_set, file_path, ped_samples)\n",
        "        elif method == 'ssd':\n",
        "            return self._generate_csv_data_ssd(image_set, file_path, ped_samples)\n",
        "\n",
        "    # def _get_data_frcnn(self, ped_samples):\n",
        "    #     \"\"\"\n",
        "    #     Data generation for Faster-rcnn algorithm\n",
        "    #     :param ped_samples: Dictionary of all samples\n",
        "    #     \"\"\"\n",
        "    #     classes_count = {}\n",
        "    #     class_mapping = {}\n",
        "    #     all_imgs = {}\n",
        "    #     class_name = 'pedestrian'\n",
        "    #     classes_count['bg'] = 0\n",
        "    #     class_mapping['bg'] = 1\n",
        "    #     classes_count[class_name] = 0\n",
        "    #     class_mapping[class_name] = 0\n",
        "\n",
        "    #     for img, samples in sorted(ped_samples.items()):\n",
        "    #         if not samples:\n",
        "    #             continue\n",
        "    #         all_imgs[img] = {'filepath': img, 'width': samples[0]['width'],\n",
        "    #                          'height': samples[0]['height'], 'bboxes': []}\n",
        "    #         for s in samples:\n",
        "    #             box = s['box']\n",
        "    #             all_imgs[img]['bboxes'].append({'class': class_name, 'x1': box[0],\n",
        "    #                                             'x2': box[2], 'y1': box[1], 'y2': box[3]})\n",
        "    #     print('Data generated for Faster-rcnn')\n",
        "    #     all_data = []\n",
        "    #     for key in all_imgs:\n",
        "    #         all_data.append(all_imgs[key])\n",
        "    #     return all_data, classes_count, class_mapping\n",
        "\n",
        "    def _generate_csv_data_retinanet(self, image_set, file_path, ped_samples):\n",
        "        \"\"\"\n",
        "        Data generation for Retinanet algorithm\n",
        "        :param image_set: Data split\n",
        "        :param file_path: Path to save the data\n",
        "        :param ped_samples: Dictionary of all samples\n",
        "        \"\"\"\n",
        "        class_name = 'pedestrian'\n",
        "        data_save_path = file_path + 'retinanet_' + image_set + '.csv'\n",
        "        with open(data_save_path, \"wt\") as f:\n",
        "            for img, samples in sorted(ped_samples.items()):\n",
        "                if not samples:\n",
        "                    f.write('%s,,,,,\\n' % (img))\n",
        "                for s in samples:\n",
        "                    box = s['box']\n",
        "                    f.write('%s,%.0f,%.0f,%.0f,%.0f,%s\\n' % (img, box[0], box[1], box[2], box[3], class_name))\n",
        "            print('Data generated for Retinanet')\n",
        "\n",
        "            map_path = file_path + '_mapping.csv'\n",
        "            with open(map_path, \"w\") as f:\n",
        "                f.write('%s,0\\n' % (class_name))\n",
        "        return data_save_path, map_path\n",
        "\n",
        "    # def _generate_csv_data_yolo3(self, image_set, file_path, ped_samples):\n",
        "    #     \"\"\"\n",
        "    #     Data generation for YOLO3 algorithm\n",
        "    #     :param image_set: Data split\n",
        "    #     :param file_path: Path to save the data\n",
        "    #     :param ped_samples: Dictionary of all samples\n",
        "    #     \"\"\"\n",
        "    #     class_name = 'pedestrian'\n",
        "    #     all_imgs = {}\n",
        "    #     data_save_path = file_path + 'yolo3_' + image_set + '.txt'\n",
        "    #     with open(data_save_path, \"wt\") as f:\n",
        "    #         for img, samples in sorted(ped_samples.items()):\n",
        "    #             if not samples:\n",
        "    #                 continue\n",
        "    #             f.write('%s ' % (img))\n",
        "    #             for s in samples:\n",
        "    #                 box = s['box']\n",
        "    #                 f.write('%.0f,%.0f,%.0f,%.0f,%.0f ' % (box[0], box[1], box[2], box[3], 0))\n",
        "    #             f.write('\\n')\n",
        "    #         print('Data generated for YOLO3')\n",
        "    #     map_path = file_path + 'mapping_yolo3'\n",
        "    #     with open(map_path, \"wt\") as f:\n",
        "    #         f.write('%s,0\\n' % (class_name))\n",
        "    #     return data_save_path, map_path\n",
        "\n",
        "    # def _generate_csv_data_ssd(self, image_set, file_path, ped_samples):\n",
        "    #     \"\"\"\n",
        "    #     Data generation for SSD algorithm\n",
        "    #     :param image_set: Data split\n",
        "    #     :param file_path: Path to save the data\n",
        "    #     :param ped_samples: Dictionary of all samples\n",
        "    #     \"\"\"\n",
        "    #     data_save_path = file_path + 'ssd_' + image_set + '.csv'\n",
        "    #     with open(data_save_path, \"wt\") as f:\n",
        "    #         for img, samples in sorted(ped_samples.items()):\n",
        "    #             if not samples:\n",
        "    #                 continue\n",
        "    #             for s in samples:\n",
        "    #                 box = s['box']\n",
        "    #                 f.write('%s,%.0f,%.0f,%.0f,%.0f,%s\\n' % (img, box[0], box[1], box[2], box[3], 1))\n",
        "    #         print('Data generated for SSD')\n",
        "    #     return data_save_path\n",
        "\n",
        "    # Trajectory data generation\n",
        "    def _get_data_ids(self, image_set, params):\n",
        "        \"\"\"\n",
        "        A helper function to generate set id and ped ids (if needed) for processing\n",
        "        :param image_set: Image-set to generate data\n",
        "        :param params: Data generation params\n",
        "        :return: Set and pedestrian ids\n",
        "        \"\"\"\n",
        "        _pids = None\n",
        "\n",
        "        if params['data_split_type'] == 'default':\n",
        "            return self._get_video_ids_split(image_set, params['subset']), _pids\n",
        "\n",
        "        video_ids = self._get_video_ids_split('all', params['subset'])\n",
        "        if params['data_split_type'] == 'random':\n",
        "            params['random_params']['sample_type'] = params['sample_type']\n",
        "            _pids = self._get_random_pedestrian_ids(image_set, **params['random_params'])\n",
        "        elif params['data_split_type'] == 'kfold':\n",
        "            params['kfold_params']['sample_type'] = params['sample_type']\n",
        "            _pids = self._get_kfold_pedestrian_ids(image_set, **params['kfold_params'])\n",
        "\n",
        "        return video_ids, _pids\n",
        "\n",
        "    def _height_check(self, height_rng, frame_ids, boxes, images, occlusion):\n",
        "        \"\"\"\n",
        "        Checks whether the bounding boxes are within a given height limit. If not, it\n",
        "        will adjust the length of data sequences accordingly\n",
        "        :param height_rng: Height limit [lower, higher]\n",
        "        :param frame_ids: List of frame ids\n",
        "        :param boxes: List of bounding boxes\n",
        "        :param images: List of images\n",
        "        :param occlusion: List of occlusions\n",
        "        :return: The adjusted data sequences\n",
        "        \"\"\"\n",
        "        imgs, box, frames, occ = [], [], [], []\n",
        "        for i, b in enumerate(boxes):\n",
        "            bbox_height = abs(b[0] - b[2])\n",
        "            if height_rng[0] <= bbox_height <= height_rng[1]:\n",
        "                box.append(b)\n",
        "                imgs.append(images[i])\n",
        "                frames.append(frame_ids[i])\n",
        "                occ.append(occlusion[i])\n",
        "        return imgs, box, frames, occ\n",
        "\n",
        "    def _get_center(self, box):\n",
        "        \"\"\"\n",
        "        Calculates the center coordinate of a bounding box\n",
        "        :param box: Bounding box coordinates\n",
        "        :return: The center coordinate\n",
        "        \"\"\"\n",
        "        return [(box[0] + box[2]) / 2, (box[1] + box[3]) / 2]\n",
        "\n",
        "    def generate_data_trajectory_sequence(self, image_set, **opts):\n",
        "        \"\"\"\n",
        "        Generates pedestrian tracks\n",
        "        :param image_set: the split set to produce for. Options are train, test, val.\n",
        "        :param opts:\n",
        "                'fstride': Frequency of sampling from the data.\n",
        "                'sample_type': Whether to use 'all' pedestrian annotations or the ones\n",
        "                                    with 'beh'avior only.\n",
        "                'subset': The subset of data annotations to use. Options are: 'default': Includes high resolution and\n",
        "                                                                                         high visibility videos\n",
        "                                                                           'high_visibility': Only videos with high\n",
        "                                                                                             visibility (include low\n",
        "                                                                                              resolution videos)\n",
        "                                                                           'all': Uses all videos\n",
        "                'height_rng': The height range of pedestrians to use.\n",
        "                'squarify_ratio': The width/height ratio of bounding boxes. A value between (0,1]. 0 the original\n",
        "                                        ratio is used.\n",
        "                'data_split_type': How to split the data. Options: 'default', predefined sets, 'random', randomly split the data,\n",
        "                                        and 'kfold', k-fold data split (NOTE: only train/test splits).\n",
        "                'seq_type': Sequence type to generate. Options: 'trajectory', generates tracks, 'crossing', generates\n",
        "                                  tracks up to 'crossing_point', 'intention' generates tracks similar to human experiments\n",
        "                'min_track_size': Min track length allowable.\n",
        "                'random_params: Parameters for random data split generation. (see _get_random_pedestrian_ids)\n",
        "                'kfold_params: Parameters for kfold split generation. (see _get_kfold_pedestrian_ids)\n",
        "        :return: Sequence data\n",
        "        \"\"\"\n",
        "        params = {'fstride': 1,\n",
        "                  'sample_type': 'all',  # 'beh'\n",
        "                  'subset': 'default',\n",
        "                  'height_rng': [0, float('inf')],\n",
        "                  'squarify_ratio': 0,\n",
        "                  'data_split_type': 'default',  # kfold, random, default\n",
        "                  'seq_type': 'intention',\n",
        "                  'min_track_size': 15,\n",
        "                  'random_params': {'ratios': None,\n",
        "                                    'val_data': True,\n",
        "                                    'regen_data': False},\n",
        "                  'kfold_params': {'num_folds': 5, 'fold': 1}}\n",
        "        assert all(k in params for k in opts.keys()), \"Wrong option(s).\"\\\n",
        "        \"Choose one of the following: {}\".format(list(params.keys()))\n",
        "        params.update(opts)\n",
        "\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Generating action sequence data\")\n",
        "        self._print_dict(params)\n",
        "\n",
        "        annot_database = self.generate_database()\n",
        "        if params['seq_type'] == 'trajectory':\n",
        "            sequence = self._get_trajectories(image_set, annot_database, **params)\n",
        "        elif params['seq_type'] == 'crossing':\n",
        "            sequence = self._get_crossing(image_set, annot_database, **params)\n",
        "        elif params['seq_type'] == 'intention':\n",
        "            sequence = self._get_intention(image_set, annot_database, **params)\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    def _get_trajectories(self, image_set, annotations, **params):\n",
        "        \"\"\"\n",
        "        Generates trajectory data.\n",
        "        :param params: Parameters for generating trajectories\n",
        "        :param annotations: The annotations database\n",
        "        :return: A dictionary of trajectories\n",
        "        \"\"\"\n",
        "\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Generating trajectory data\")\n",
        "\n",
        "        num_pedestrians = 0\n",
        "        seq_stride = params['fstride']\n",
        "        sq_ratio = params['squarify_ratio']\n",
        "        height_rng = params['height_rng']\n",
        "\n",
        "        image_seq, pids_seq = [], []\n",
        "        box_seq, center_seq, occ_seq = [], [], []\n",
        "        intent_seq = []\n",
        "        vehicle_seq = []\n",
        "        resolution_seq = []\n",
        "        video_ids, _pids = self._get_data_ids(image_set, params)\n",
        "\n",
        "        for vid in sorted(video_ids):\n",
        "            img_width = annotations[vid]['width']\n",
        "            img_height = annotations[vid]['height']\n",
        "            pid_annots = annotations[vid]['ped_annotations']\n",
        "            vid_annots = annotations[vid]['vehicle_annotations']\n",
        "\n",
        "            for pid in sorted(annotations[vid]['ped_annotations']):\n",
        "                if params['data_split_type'] != 'default' and pid not in _pids:\n",
        "                    continue\n",
        "                if 'p' in pid:\n",
        "                    continue\n",
        "                if params['sample_type'] == 'beh' and 'b' not in pid:\n",
        "                    continue\n",
        "                num_pedestrians += 1\n",
        "                frame_ids = pid_annots[pid]['frames']\n",
        "                images = [join(self._jaad_path, 'images', vid, '{:05d}.png'.format(f)) for f in\n",
        "                          pid_annots[pid]['frames']]\n",
        "                boxes = pid_annots[pid]['bbox']\n",
        "                occlusions = pid_annots[pid]['occlusion']\n",
        "\n",
        "                if height_rng[0] > 0 or height_rng[1] < float('inf'):\n",
        "                    images, boxes, frame_ids, occlusions = self._height_check(height_rng,\n",
        "                                                                              frame_ids, boxes,\n",
        "                                                                              images, occlusions)\n",
        "\n",
        "                if len(boxes) / seq_stride < params['min_track_size']:\n",
        "                    continue\n",
        "\n",
        "                if sq_ratio:\n",
        "                    boxes = [self._squarify(b, sq_ratio, img_width) for b in boxes]\n",
        "\n",
        "                ped_ids = [[pid]] * len(boxes)\n",
        "\n",
        "                if params['sample_type'] == 'all':\n",
        "                    intent = [[0]] * len(boxes)\n",
        "                else:\n",
        "                    if annotations[vid]['ped_annotations'][pid]['attributes']['crossing'] == -1:\n",
        "                        intent = [[0]] * len(boxes)\n",
        "                    else:\n",
        "                        intent = [[1]] * len(boxes)\n",
        "                center = [self._get_center(b) for b in boxes]\n",
        "\n",
        "                occ_seq.append(occlusions[::seq_stride])\n",
        "                image_seq.append(images[::seq_stride])\n",
        "                box_seq.append(boxes[::seq_stride])\n",
        "                center_seq.append(center[::seq_stride])\n",
        "                intent_seq.append(intent[::seq_stride])\n",
        "                pids_seq.append(ped_ids[::seq_stride])\n",
        "                vehicle_seq.append([[vid_annots[i]]\n",
        "                                    for i in frame_ids][::seq_stride])\n",
        "                resolutions = [[img_width, img_height]] * len(boxes)\n",
        "                resolution_seq.append(resolutions[::seq_stride])\n",
        "\n",
        "        print('Split: %s' % image_set)\n",
        "        print('Number of pedestrians: %d ' % num_pedestrians)\n",
        "        print('Total number of used pedestrians: %d ' % len(image_seq))\n",
        "\n",
        "        return {'image': image_seq,\n",
        "                'resolution': resolution_seq,\n",
        "                'pid': pids_seq,\n",
        "                'bbox': box_seq,\n",
        "                'center': center_seq,\n",
        "                'occlusion': occ_seq,\n",
        "                'intent': intent_seq,\n",
        "                'vehicle_act': vehicle_seq}\n",
        "\n",
        "    def _get_crossing(self, image_set, annotations, **params):\n",
        "        \"\"\"\n",
        "        Generates crossing data.\n",
        "        :param image_set: Data split to use\n",
        "        :param annotations: Annotations database\n",
        "        :param params: Parameters to generate data (see generade_database)\n",
        "        :return: A dictionary of trajectories\n",
        "        \"\"\"\n",
        "\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Generating crossing data\")\n",
        "\n",
        "        num_pedestrians = 0\n",
        "        seq_stride = params['fstride']\n",
        "        sq_ratio = params['squarify_ratio']\n",
        "        height_rng = params['height_rng']\n",
        "        image_seq, pids_seq = [], []\n",
        "        box_seq, center_seq, occ_seq = [], [], []\n",
        "        intent_seq = []\n",
        "        vehicle_seq = []\n",
        "        activities = []\n",
        "\n",
        "        video_ids, _pids = self._get_data_ids(image_set, params)\n",
        "\n",
        "        for vid in sorted(video_ids):\n",
        "            img_width = annotations[vid]['width']\n",
        "            img_height = annotations[vid]['height']\n",
        "            pid_annots = annotations[vid]['ped_annotations']\n",
        "            vid_annots = annotations[vid]['vehicle_annotations']\n",
        "            for pid in sorted(pid_annots):\n",
        "                if params['data_split_type'] != 'default' and pid not in _pids:\n",
        "                    continue\n",
        "                if 'p' in pid:\n",
        "                    continue\n",
        "                if params['sample_type'] == 'beh' and 'b' not in pid:\n",
        "                    continue\n",
        "                num_pedestrians += 1\n",
        "\n",
        "                frame_ids = pid_annots[pid]['frames']\n",
        "\n",
        "                if 'b' in pid:\n",
        "                    event_frame = pid_annots[pid]['attributes']['crossing_point']\n",
        "                else:\n",
        "                    event_frame = -1\n",
        "\n",
        "                if event_frame == -1:\n",
        "                    end_idx = -3\n",
        "                else:\n",
        "                   end_idx = frame_ids.index(event_frame)\n",
        "                boxes = pid_annots[pid]['bbox'][:end_idx + 1]\n",
        "                frame_ids = frame_ids[: end_idx + 1]\n",
        "                images = [self._get_image_path(vid, f) for f in frame_ids]\n",
        "                occlusions = pid_annots[pid]['occlusion'][:end_idx + 1]\n",
        "\n",
        "                if height_rng[0] > 0 or height_rng[1] < float('inf'):\n",
        "                    images, boxes, frame_ids, occlusions = self._height_check(height_rng,\n",
        "                                                                              frame_ids, boxes,\n",
        "                                                                              images, occlusions)\n",
        "\n",
        "                if len(boxes) / seq_stride < params['min_track_size']:\n",
        "                    continue\n",
        "\n",
        "                if sq_ratio:\n",
        "                    boxes = [self._squarify(b, sq_ratio, img_width) for b in boxes]\n",
        "\n",
        "                image_seq.append(images[::seq_stride])\n",
        "                box_seq.append(boxes[::seq_stride])\n",
        "                center_seq.append([self._get_center(b) for b in boxes][::seq_stride])\n",
        "                occ_seq.append(occlusions[::seq_stride])\n",
        "\n",
        "                ped_ids = [[pid]] * len(boxes)\n",
        "                pids_seq.append(ped_ids[::seq_stride])\n",
        "\n",
        "                if 'b' not in pid:\n",
        "                    intent = [[0]] * len(boxes)\n",
        "                    acts = [[0]] * len(boxes)\n",
        "                else:\n",
        "                    if annotations[vid]['ped_annotations'][pid]['attributes']['crossing'] == -1:\n",
        "                        intent = [[0]] * len(boxes)\n",
        "                    else:\n",
        "                        intent = [[1]] * len(boxes)\n",
        "                    acts = [[int(pid_annots[pid]['attributes']['crossing'] > 0)]] * len(boxes)\n",
        "\n",
        "                intent_seq.append(intent[::seq_stride])\n",
        "                activities.append(acts[::seq_stride])\n",
        "                vehicle_seq.append([[vid_annots[i]]\n",
        "                                    for i in frame_ids][::seq_stride])\n",
        "\n",
        "        print('Split: %s' % image_set)\n",
        "        print('Number of pedestrians: %d ' % num_pedestrians)\n",
        "        print('Total number of samples: %d ' % len(image_seq))\n",
        "\n",
        "        return {'image': image_seq,\n",
        "                'pid': pids_seq,\n",
        "                'bbox': box_seq,\n",
        "                'center': center_seq,\n",
        "                'occlusion': occ_seq,\n",
        "                'vehicle_act': vehicle_seq,\n",
        "                'intent': intent_seq,\n",
        "                'activities': activities,\n",
        "                'image_dimension': (img_width, img_height)}\n",
        "\n",
        "    def _get_intention(self, image_set, annotations, **params):\n",
        "        \"\"\"\n",
        "        Generates intention data.\n",
        "        :param image_set: Data split to use\n",
        "        :param annotations: Annotations database\n",
        "        :param params: Parameters to generate data (see generade_database)\n",
        "        :return: A dictionary of trajectories\n",
        "        \"\"\"\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Generating intention data\")\n",
        "\n",
        "        num_pedestrians = 0\n",
        "        seq_stride = params['fstride']\n",
        "        sq_ratio = params['squarify_ratio']\n",
        "        height_rng = params['height_rng']\n",
        "        image_seq, pids_seq = [], []\n",
        "        box_seq, center_seq, occ_seq = [], [], []\n",
        "        intent_seq = []\n",
        "        video_ids, _pids = self._get_data_ids(image_set, params)\n",
        "\n",
        "        for vid in sorted(video_ids):\n",
        "            img_width = annotations[vid]['width']\n",
        "            pid_annots = annotations[vid]['ped_annotations']\n",
        "            for pid in sorted(pid_annots):\n",
        "                if params['data_split_type'] != 'default' and pid not in _pids:\n",
        "                    continue\n",
        "                if 'p' in pid:\n",
        "                    continue\n",
        "                if params['sample_type'] == 'beh' and 'b' not in pid:\n",
        "                    continue\n",
        "                num_pedestrians += 1\n",
        "                frame_ids = pid_annots[pid]['frames']\n",
        "\n",
        "                if params['sample_type'] == 'beh':\n",
        "                    event_frame = pid_annots[pid]['attributes']['decision_point']\n",
        "                else:\n",
        "                    event_frame = -1\n",
        "\n",
        "                if event_frame == -1:\n",
        "                    end_idx = -3\n",
        "                else:\n",
        "                    end_idx = frame_ids.index(event_frame)\n",
        "\n",
        "                boxes = pid_annots[pid]['bbox'][:end_idx + 1]\n",
        "                frame_ids = frame_ids[: end_idx + 1]\n",
        "                images = [self._get_image_path(vid, f) for f in frame_ids]\n",
        "                occlusions = pid_annots[pid]['occlusion'][:end_idx + 1]\n",
        "\n",
        "                if height_rng[0] > 0 or height_rng[1] < float('inf'):\n",
        "                    images, boxes, frame_ids, occlusions = self._height_check(height_rng,\n",
        "                                                                              frame_ids, boxes,\n",
        "                                                                              images, occlusions)\n",
        "                if len(boxes) / seq_stride < params['min_track_size']:\n",
        "                    continue\n",
        "\n",
        "                if sq_ratio:\n",
        "                    boxes = [self._squarify(b, sq_ratio, img_width) for b in boxes]\n",
        "\n",
        "                center_seq.append([self._get_center(b) for b in boxes][::seq_stride])\n",
        "                image_seq.append(images[::seq_stride])\n",
        "                box_seq.append(boxes[::seq_stride])\n",
        "                occ_seq.append(occlusions[::seq_stride])\n",
        "                ped_ids = [[pid]] * len(boxes)\n",
        "                pids_seq.append(ped_ids[::seq_stride])\n",
        "\n",
        "                if params['sample_type'] == 'all':\n",
        "                    intent = [[0]] * len(boxes)\n",
        "                else:\n",
        "                    if annotations[vid]['ped_annotations'][pid]['attributes']['crossing'] == -1:\n",
        "                        intent = [[0]] * len(boxes)\n",
        "                    else:\n",
        "                        intent = [[1]] * len(boxes)\n",
        "                intent_seq.append(intent[::seq_stride])\n",
        "\n",
        "        print('Split: %s' % image_set)\n",
        "        print('Number of pedestrians: %d ' % num_pedestrians)\n",
        "        print('Total number of samples: %d ' % len(image_seq))\n",
        "\n",
        "        return {'image': image_seq,\n",
        "                'pid': pids_seq,\n",
        "                'bbox': box_seq,\n",
        "                'center': center_seq,\n",
        "                'occlusion': occ_seq,\n",
        "                'intent': intent_seq}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 정제"
      ],
      "metadata": {
        "id": "EMIwf7bBtaNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kBDJ4j2CiSR"
      },
      "outputs": [],
      "source": [
        "    # def get_detection_data(self, image_set, method, occlusion_type=None, file_path='data/', **params):\n",
        "    #     \"\"\"\n",
        "    #     Generates data for pedestrian detection algorithms\n",
        "    #     :param image_set: Split set name\n",
        "    #     :param method: Detection algorithm: frcnn, retinanet, yolo3, ssd\n",
        "    #     :param occlusion_type: The types of occlusion: None: only unoccluded samples\n",
        "    #                                                    part: Unoccluded and partially occluded samples\n",
        "    #                                                    full: All samples\n",
        "    #     :param file_path: Where to save the script file\n",
        "    #     :return: Pedestrian samples\n",
        "    #     \"\"\"\n",
        "    #     squarify_ratio = params['squarify_ratio']\n",
        "    #     frame_stride = params['fstride']\n",
        "    #     height_rng = params['height_rng']\n",
        "    #     if not exists(file_path):\n",
        "    #         makedirs(file_path)\n",
        "    #     if height_rng is None:\n",
        "    #         height_rng = [0, float('inf')]\n",
        "\n",
        "    #     annotations = self.generate_database()\n",
        "    #     video_ids, _pids = self._get_data_ids(image_set, params)\n",
        "\n",
        "    #     ped_samples = {}\n",
        "    #     unique_samples = []\n",
        "    #     total_sample_count = 0\n",
        "    #     for vid in video_ids:\n",
        "    #         img_width = annotations[vid]['width']\n",
        "    #         img_height = annotations[vid]['height']\n",
        "    #         num_frames = annotations[vid]['num_frames']\n",
        "    #         for i in range(0,num_frames,frame_stride):\n",
        "    #             ped_samples[join(self._jaad_path, 'images', vid, '{:05d}.png'.format(i))] = []\n",
        "    #         for pid in annotations[vid]['ped_annotations']:\n",
        "    #             if params['data_split_type'] != 'default' and pid not in _pids:\n",
        "    #                 continue\n",
        "    #             difficult =  0\n",
        "    #             if 'p' in pid:\n",
        "    #                 difficult = -1\n",
        "    #                 if image_set in ['train', 'val']:\n",
        "    #                     continue\n",
        "    #             imgs = [join(self._jaad_path, 'images', vid, '{:05d}.png'.format(f)) for f in \\\n",
        "    #                         annotations[vid]['ped_annotations'][pid]['frames']]\n",
        "    #             boxes = annotations[vid]['ped_annotations'][pid]['bbox']\n",
        "    #             occlusion = annotations[vid]['ped_annotations'][pid]['occlusion']\n",
        "\n",
        "    #             # 관련 행동 레이블 추출\n",
        "    #             behavior_labels = [annotations[vid]['ped_annotations'][pid]['action'][i] for i, b in enumerate(boxes)]  # 예시: 'action' 속성 사용\n",
        "    #             attributes = [annotations[vid]['ped_annotations'][pid]['attribute'][i] for i, b in enumerate(boxes)]  # 속성 추가\n",
        "\n",
        "    #             for i, b in enumerate(boxes):\n",
        "    #                 if imgs[i] not in ped_samples:\n",
        "    #                     continue\n",
        "    #                 bbox_height = abs(b[0] - b[2])\n",
        "    #                 if height_rng[0] <= bbox_height <= height_rng[1]:\n",
        "    #                     if (occlusion_type == None and occlusion[i] == 0) or \\\n",
        "    #                             (occlusion_type == 'part' and occlusion[i] < 2) or \\\n",
        "    #                             (occlusion_type == 'full'):\n",
        "    #                         if squarify_ratio:\n",
        "    #                             b = self._squarify(b, squarify_ratio, img_width)\n",
        "    #                         # 샘플에 attribute 추가\n",
        "    #                         ped_samples[imgs[i]].append({\n",
        "    #                             'width': img_width,\n",
        "    #                             'height': img_height,\n",
        "    #                             'tag': pid,\n",
        "    #                             'box': b,\n",
        "    #                             'seg_area': (b[2] - b[0] + 1) * (b[3] - b[1] + 1),\n",
        "    #                             'occlusion': occlusion[i],\n",
        "    #                             'difficult': difficult,\n",
        "    #                             'attribute': attributes[i],\n",
        "    #                         })\n",
        "    #                         if pid not in unique_samples:\n",
        "    #                             unique_samples.append(pid)\n",
        "    #                         total_sample_count += 1\n",
        "    #     print('Number of unique pedestrians %d ' % len(unique_samples))\n",
        "    #     print('Number of samples %d ' % total_sample_count)\n",
        "    #     if method == 'frcnn':\n",
        "    #         return self._get_data_frcnn(ped_samples)\n",
        "    #     elif method == 'retinanet':\n",
        "    #         return self._generate_csv_data_retinanet(image_set, file_path, ped_samples)\n",
        "    #     elif method == 'yolo3':\n",
        "    #         return self._generate_csv_data_yolo3(image_set, file_path, ped_samples)\n",
        "    #     elif method == 'ssd':\n",
        "    #         return self._generate_csv_data_ssd(image_set, file_path, ped_samples)\n",
        "    # def _generate_csv_data_retinanet(self, image_set, file_path, ped_samples):\n",
        "    #     \"\"\"\n",
        "    #     Data generation for Retinanet algorithm\n",
        "    #     :param image_set: Data split\n",
        "    #     :param file_path: Path to save the data\n",
        "    #     :param ped_samples: Dictionary of all samples\n",
        "    #     \"\"\"\n",
        "    #     class_name = 'pedestrian'\n",
        "    #     data_save_path = file_path + 'retinanet_' + image_set + '.csv'\n",
        "    #     with open(data_save_path, \"wt\") as f:\n",
        "    #         for img, samples in sorted(ped_samples.items()):\n",
        "    #             if not samples:\n",
        "    #                 f.write('%s,,,,,\\n' % (img))\n",
        "\n",
        "    #             for s in samples:\n",
        "    #                 box = s['box']\n",
        "    #                 attribute = s['attribute']\n",
        "\n",
        "    #                 # CSV 파일에 attribute 추가\n",
        "    #                 f.write('%s,%.0f,%.0f,%.0f,%.0f,%s,%s\\n' % (img, box[0], box[1], box[2], box[3], class_name, attribute))\n",
        "\n",
        "    #         print('Data generated for Retinanet')\n",
        "\n",
        "    #         map_path = file_path + '_mapping.csv'\n",
        "    #         with open(map_path, \"w\") as f:\n",
        "    #             f.write('%s,0\\n' % (class_name))\n",
        "    #     return data_save_path, map_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def main():\n",
        "#     # 데이터셋의 경로와 다른 설정을 원하는대로 수정합니다.\n",
        "#     jaad_dataset = JAAD(data_path='/content/drive/MyDrive/MSc_Project/JAAD_dataset', regen_pkl=True)\n",
        "\n",
        "#     # 데이터셋을 생성하고 저장합니다.\n",
        "#     jaad_dataset.extract_and_save_images()\n",
        "\n",
        "#     # 데이터셋에서 특정 작업을 수행합니다.\n",
        "#     jaad_dataset.get_data_stats()  # 데이터셋의 통계 정보를 출력합니다.\n",
        "\n",
        "#     # get_intention 반환값 가져오기\n",
        "#     intention_data = jaad_dataset.get_intention(image_set='train', params={'fstride': 1, 'squarify_ratio': 1.0, 'height_rng': [0, float('inf')], 'min_track_size': 1, 'sample_type': 'beh', 'data_split_type': 'default'})\n",
        "\n",
        "#     # 여기서 intention_data를 사용하여 작업을 수행합니다.\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "FgtouO3fu0YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from timm.models import create_model\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from lxml import etree\n",
        "\n",
        "# ViViT 모델 설정\n",
        "MODEL_NAME = 'deit_base_distilled_patch16_224'\n",
        "NUM_CLASSES = 3  # 예시: 'standing', 'walking', 'crossing'\n",
        "\n",
        "# 데이터셋 경로\n",
        "VIDEO_PATH = 'JAAD/video'\n",
        "ANNOTATION_PATH = 'JAAD/annotation'\n",
        "\n",
        "# 모델 불러오기\n",
        "model = create_model(MODEL_NAME, num_classes=NUM_CLASSES, pretrained=True)\n",
        "\n",
        "# 데이터 로더\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 추론 함수\n",
        "def inference(video_path, annotation_path, model, transform):\n",
        "    # 영상 캡처\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # 프레임별 예측 결과 저장\n",
        "    predictions = []\n",
        "\n",
        "    # 프레임 순회\n",
        "    while True:\n",
        "        # 프레임 읽기\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # 이미지 전처리\n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        image = transform(image)\n",
        "\n",
        "        # 모델 예측\n",
        "        with torch.no_grad():\n",
        "            output = model(image.unsqueeze(0))\n",
        "            prediction = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        # 예측 결과 저장\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# 모든 영상 추론\n",
        "for video_file in tqdm(os.listdir(VIDEO_PATH)):\n",
        "    video_path = os.path.join(VIDEO_PATH, video_file)\n",
        "    annotation_path = os.path.join(ANNOTATION_PATH, video_file.replace('.mp4', '.xml'))\n",
        "\n",
        "    # XML 파일 읽어 프레임별 정보 추출\n",
        "    frames = parse_xml(annotation_path)\n",
        "\n",
        "    # 추론 수행\n",
        "    predictions = inference(video_path, annotation_path, model, transform)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f'영상: {video_file}')\n",
        "    for i, prediction in enumerate(predictions):\n",
        "        frame_info = frames[i + 1]  # 프레임 ID는 1부터 시작\n",
        "        bbox = frame_info['bboxes'][0]  # 예시: 하나의 객체만 추출\n",
        "        attribute = frame_info['attributes'][0]\n",
        "        print(f'프레임 {i:05d}: {prediction}')\n",
        "        print(f'    - Bounding box: {bbox}')\n",
        "        print(f'    - Attribute: {attribute}')\n",
        "\n",
        "print('Human Behavior Estimation 완료!')\n"
      ],
      "metadata": {
        "id": "FlPYh7bpyF0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4차 수정"
      ],
      "metadata": {
        "id": "BC_KO5md0GFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcI8Tyfh06uc",
        "outputId": "c7ed9732-81f1-4c83-848f-8c7a2cad01be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m548.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->timm)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 timm-0.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "UNbZXVht1kOx",
        "outputId": "11fe1fe9-21d9-4253-ed67-24b75c819242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.25.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Installing collected packages: tensorboard\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.16.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard"
                ]
              },
              "id": "3ff6cd7cea38413399f985b91997beee"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2Vdds4J2HXu",
        "outputId": "71a50842-84ab-4a42-ed34-0c3d7eedc140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m92.2/101.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 부분만 annotation(xml파일 그대로) >> 성공\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from lxml import etree\n",
        "# from timm.models import create_model\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from lxml import etree\n",
        "# from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from os.path import join\n",
        "from lxml import etree\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "\n",
        "class JAAD:\n",
        "    def __init__(self, data_path='/content', regen_pkl=True):\n",
        "        # ... (rest of your JAAD class)\n",
        "        self._videos_path = os.path.join(data_path, 'videos')\n",
        "        self._annotation_path = os.path.join(data_path, 'annotations')\n",
        "        self._output_path = os.path.join(data_path, 'vivit_labels')\n",
        "\n",
        "    def save_vivit_labels(self, annotations, video_id):\n",
        "        \"\"\"Saves annotations in XML format\"\"\"\n",
        "        output_dir = self._output_path #os.path.join(self._output_path, video_id)\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "        output_file = os.path.join(output_dir, video_id+'.xml')\n",
        "\n",
        "        root = ET.Element('video', attrib={'id': video_id})\n",
        "\n",
        "        for ped_id in annotations['ped_annotations']:\n",
        "            ped_data = annotations['ped_annotations'][ped_id]\n",
        "            pedestrian = ET.SubElement(root, 'pedestrian', attrib={'id': ped_id})\n",
        "\n",
        "            for frame_idx in range(len(ped_data['frames'])):\n",
        "                frame = ET.SubElement(pedestrian, 'frame', attrib={'number': str(ped_data['frames'][frame_idx])})\n",
        "\n",
        "                bbox = ET.SubElement(frame, 'bbox')\n",
        "                bbox.set('xtl', str(ped_data['bbox'][frame_idx][0]))\n",
        "                bbox.set('ytl', str(ped_data['bbox'][frame_idx][1]))\n",
        "                bbox.set('xbr', str(ped_data['bbox'][frame_idx][2]))\n",
        "                bbox.set('ybr', str(ped_data['bbox'][frame_idx][3]))\n",
        "\n",
        "                ET.SubElement(frame, 'occlusion').text = str(ped_data['occlusion'][frame_idx])\n",
        "\n",
        "                if 'behavior' in ped_data:\n",
        "                    behavior = ET.SubElement(frame, 'behavior')\n",
        "                    for beh in ped_data['behavior']:\n",
        "                        ET.SubElement(behavior, beh).text = str(ped_data['behavior'][beh][frame_idx])\n",
        "\n",
        "        # Create a well-formatted XML output\n",
        "        tree = ET.ElementTree(root)\n",
        "        ET.indent(tree, space=\"  \", level=0)  # For pretty-printing\n",
        "\n",
        "        tree.write(output_file, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "    def _map_text_to_scalar(self, label_type, value):\n",
        "        \"\"\"\n",
        "        Maps a text label in XML file to scalars\n",
        "        :param label_type: The label type\n",
        "        :param value: The text to be mapped\n",
        "        :return: The scalar value\n",
        "        \"\"\"\n",
        "        map_dic = {'occlusion': {'none': 0, 'part': 1, 'full': 2},\n",
        "                    'action': {'standing': 0, 'walking': 1},\n",
        "                    'nod': {'__undefined__': 0, 'nodding': 1},\n",
        "                    'look': {'not-looking': 0, 'looking': 1},\n",
        "                    'hand_gesture': {'__undefined__': 0, 'greet': 1, 'yield': 2,\n",
        "                                    'rightofway': 3, 'other': 4},\n",
        "                    'reaction': {'__undefined__': 0, 'clear_path': 1, 'speed_up': 2,\n",
        "                                'slow_down': 3},\n",
        "                    'cross': {'not-crossing': 0, 'crossing': 1, 'irrelevant': -1},\n",
        "                    'age': {'child': 0, 'young': 1, 'adult': 2, 'senior': 3},\n",
        "                    'designated': {'ND': 0, 'D': 1},\n",
        "                    'gender': {'n/a': 0, 'female': 1, 'male': 2},\n",
        "                    'intersection': {'no': 0, 'yes': 1},\n",
        "                    'motion_direction': {'n/a': 0, 'LAT': 1, 'LONG': 2},\n",
        "                    'traffic_direction': {'OW': 0, 'TW': 1},\n",
        "                    'signalized': {'n/a': 0, 'NS': 1, 'S': 2},\n",
        "                    'vehicle': {'stopped': 0, 'moving_slow': 1, 'moving_fast': 2,\n",
        "                                'decelerating': 3, 'accelerating': 4},\n",
        "                    'road_type': {'street': 0, 'parking_lot': 1, 'garage': 2},\n",
        "                    'traffic_light': {'n/a': 0, 'red': 1, 'green': 2}}\n",
        "\n",
        "        return map_dic[label_type][value]\n",
        "\n",
        "    def _get_annotations(self, vid):\n",
        "        \"\"\"\n",
        "        Generates a dictinary of annotations by parsing the video XML file\n",
        "        :param vid: The id of video to parse\n",
        "        :return: A dictionary of annotations\n",
        "        \"\"\"\n",
        "        path_to_file = join(self._annotation_path, vid + '.xml')\n",
        "        tree = ET.parse(path_to_file)\n",
        "        ped_annt = 'ped_annotations'\n",
        "\n",
        "        annotations = {}\n",
        "        annotations['num_frames'] = int(tree.find(\"./meta/task/size\").text)\n",
        "        annotations['width'] = int(tree.find(\"./meta/task/original_size/width\").text)\n",
        "        annotations['height'] = int(tree.find(\"./meta/task/original_size/height\").text)\n",
        "        annotations[ped_annt] = {}\n",
        "\n",
        "        ped_tracks = tree.findall(\"./track\")\n",
        "\n",
        "        for t in ped_tracks:\n",
        "            boxes = t.findall('./box')\n",
        "            new_id = boxes[0].find('./attribute[@name=\\\"id\\\"]').text\n",
        "            old_id = boxes[0].find('./attribute[@name=\\\"old_id\\\"]').text\n",
        "            annotations[ped_annt][new_id] = {'old_id': old_id, 'frames': [],\n",
        "                                              'bbox': [], 'occlusion': []}\n",
        "            if 'pedestrian' in old_id:\n",
        "                annotations['ped_annotations'][new_id]['behavior'] = {'cross': [],\n",
        "                                                                      'reaction': [],\n",
        "                                                                      'hand_gesture': [],\n",
        "                                                                      'look': [],\n",
        "                                                                      'action': [],\n",
        "                                                                      'nod': []}\n",
        "            else:\n",
        "                annotations[ped_annt][new_id]['behavior'] = {}\n",
        "\n",
        "            for b in boxes:\n",
        "                annotations[ped_annt][new_id]['bbox'].append(\n",
        "                    [float(b.get('xtl')), float(b.get('ytl')),\n",
        "                      float(b.get('xbr')), float(b.get('ybr'))])\n",
        "                occ = self._map_text_to_scalar('occlusion',\n",
        "                                                b.find('./attribute[@name=\\\"occlusion\\\"]').text)\n",
        "                annotations[ped_annt][new_id]['occlusion'].append(occ)\n",
        "                annotations[ped_annt][new_id]['frames'].append(int(b.get('frame')))\n",
        "                for beh in annotations['ped_annotations'][new_id]['behavior'].keys():\n",
        "                    annotations[ped_annt][new_id]['behavior'][beh].append(\n",
        "                        self._map_text_to_scalar(beh,\n",
        "                                                  b.find('./attribute[@name=\\\"' + beh + '\\\"]').text))\n",
        "\n",
        "        return annotations\n",
        "\n",
        "    def process_dataset(self):\n",
        "        video_filenames = [f for f in os.listdir(self._videos_path) if f.endswith('.mp4')]\n",
        "\n",
        "        for video_filename in video_filenames:\n",
        "            video_id = os.path.splitext(video_filename)[0]  # Extract video_id from filename\n",
        "            annotations = self._get_annotations(video_id)\n",
        "            self.save_vivit_labels(annotations, video_id)\n",
        "\n",
        "jaad_dataset = JAAD(data_path='/content', regen_pkl=True)  # Adjust 'data_path' if needed\n",
        "jaad_dataset.process_dataset()"
      ],
      "metadata": {
        "id": "GV5ixKDP7HUJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# annotation parsing 부분에서 에러 발생 >> 경로 문제 같음 >> 경로 수정해서 다시 해보기\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from timm.models import create_model\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from lxml import etree\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from lxml import etree\n",
        "\n",
        "def split_dataset(video_path, annotation_path, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    데이터셋을 학습 및 검증 데이터셋으로 분할\n",
        "\n",
        "    Args:\n",
        "       video_path (str): 영상 파일이 저장된 경로\n",
        "       annotation_path (str): 어노테이션 파일이 저장된 경로\n",
        "       train_ratio (float): 학습 데이터셋 비율 (기본값: 0.8)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_dataset, val_dataset)\n",
        "    \"\"\"\n",
        "\n",
        "    video_files = os.listdir(video_path)\n",
        "    random.shuffle(video_files)  # 파일을 무작위로 섞기\n",
        "\n",
        "    split_index = int(len(video_files) * train_ratio)\n",
        "    train_dataset = video_files[:split_index]\n",
        "    val_dataset = video_files[split_index:]\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def parse_xml(annotation_path):\n",
        "    \"\"\"\n",
        "    JAAD annotation XML 파일을 파싱\n",
        "\n",
        "    Args:\n",
        "        annotation_path (str): XML 파일 경로\n",
        "\n",
        "    Returns:\n",
        "        list: 프레임의 정보를 담은 리스트\n",
        "              (프레임마다 'bboxes', 'attributes' 키를 가진 dict)\n",
        "    \"\"\"\n",
        "    # print(etree.tostring(root, encoding='UTF-8', pretty_print=True).decode(\"utf-8\"))\n",
        "\n",
        "    parser = etree.XMLParser(resolve_entities=False, no_network=False)\n",
        "    tree = etree.parse(annotation_path, parser)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    frames = []\n",
        "    for track in root.findall('track'):\n",
        "        if track.attrib['label'] != 'ped':  # label이 'ped'인 경우만 추출\n",
        "            continue\n",
        "\n",
        "        for box in track.findall('box'):\n",
        "            frame_info = {}\n",
        "            frame_info['bboxes'] = [[\n",
        "                int(box.attrib['xtl']),\n",
        "                int(box.attrib['ytl']),\n",
        "                int(box.attrib['xbr']),\n",
        "                int(box.attrib['ybr'])\n",
        "            ]]\n",
        "\n",
        "            attributes = []\n",
        "            for attribute in box.findall('attribute'):\n",
        "                attributes.append(attribute.attrib['name'])\n",
        "            frame_info['attributes'] = attributes\n",
        "\n",
        "            frames.append(frame_info)\n",
        "\n",
        "    return frames\n",
        "\n",
        "def extract_patch(video_path, top_left, bottom_right, num_frames=10):\n",
        "    \"\"\"\n",
        "    영상에서 여러 프레임의 패치 추출\n",
        "\n",
        "    Args:\n",
        "        video_path (str): 영상 파일 경로\n",
        "        top_left (tuple): 패치의 좌측 상단 좌표(x, y)\n",
        "        bottom_right (tuple): 패치의 우측 하단 좌표 (x, y)\n",
        "        num_frames (int): 추출할 프레임 수 (기본값: 10)\n",
        "\n",
        "    Returns:\n",
        "        list: 추출된 패치 이미지 리스트 (Tensor 형식)\n",
        "    \"\"\"\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    x1, y1 = top_left\n",
        "    x2, y2 = bottom_right\n",
        "\n",
        "    patches = []\n",
        "    for _ in range(num_frames):\n",
        "        ret, frame = cap.read()  # 영상 프레임 읽기\n",
        "        if not ret:\n",
        "            break\n",
        "        patch = frame[y1:y2, x1:x2]\n",
        "        patches.append(patch)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # BGR -> RGB로 변환 및 Tensor 형식 변환\n",
        "    patches = [cv2.cvtColor(patch, cv2.COLOR_BGR2RGB) for patch in patches]\n",
        "    # patches = [torch.from_numpy(patch).permute(2, 0, 1) for patch in patches]\n",
        "\n",
        "    # 추출된 패치의 타입 확인 및 변환\n",
        "    patches = [torch.from_numpy(patch).permute(2, 0, 1) for patch in patches]\n",
        "    for idx, patch in enumerate(patches):\n",
        "        if not isinstance(patch, torch.Tensor):\n",
        "            print(f\"Error at index {idx}: Patch is not a torch.Tensor\")\n",
        "            continue\n",
        "\n",
        "    return patches\n",
        "# 모델 설정\n",
        "NUM_CLASSES = 3  # 예시: 'standing', 'walking', 'crossing'\n",
        "EPOCHS = 10  # 예시\n",
        "batch_size = 16\n",
        "# 데이터셋 경로\n",
        "VIDEO_PATH = '/content/videos'\n",
        "ANNOTATION_PATH = '/content/vivit_labels'\n",
        "\n",
        "# 학습 및 검증 데이터셋 분할\n",
        "train_dataset, val_dataset = split_dataset(VIDEO_PATH, ANNOTATION_PATH)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 모델 정의\n",
        "class ViViT(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # **Patch Embedding**\n",
        "        self.patch_embed = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "\n",
        "        # **Transformer Encoder**\n",
        "        self.encoder = nn.Transformer(\n",
        "            d_model=64,\n",
        "            nhead=4,\n",
        "            num_encoder_layers=6,\n",
        "            dim_feedforward=256,\n",
        "            dropout=0.1,\n",
        "            activation='relu',\n",
        "            batch_first = True\n",
        "        )\n",
        "\n",
        "        # **Classification Head**\n",
        "        self.classifier = nn.Linear(in_features=64, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        x = x.permute(0, 2, 3, 1)  # [B, H, W, C] -> [B, C, H, W]\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # Classification Head\n",
        "        x = x.mean(dim=1)  # CLS 토큰만 사용\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 모델 생성\n",
        "model = ViViT(num_classes=NUM_CLASSES)\n",
        "\n",
        "# 손실 함수 및 Optimizer 정의\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# 학습 루프\n",
        "writer = SummaryWriter()\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch_idx, video_paths in enumerate(tqdm(train_loader)):\n",
        "        # 데이터 로드\n",
        "        patches = []\n",
        "        # for video_path, annotation in zip(video_paths, annotations):\n",
        "        #     frames = parse_xml(annotation)\n",
        "        for video_path in video_paths:\n",
        "            annotation_path = video_path.replace('.mp4', '.xml')  # 추측: 어노테이션 파일 경로 생성\n",
        "            frames = parse_xml(annotation_path)\n",
        "            for frame_info in frames:\n",
        "                bboxes = frame_info['bboxes']\n",
        "                for bbox in bboxes:\n",
        "                    x1, y1, x2, y2 = bbox\n",
        "                    patch = extract_patch(video_path, (x1, y1), (x2, y2))\n",
        "                    patches.append(patch)\n",
        "\n",
        "        # 데이터 변환\n",
        "        patches = [torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(patch) for patch in patches]\n",
        "        patches = torch.stack(patches, dim=0)\n",
        "\n",
        "        # 레이블 생성\n",
        "        labels = torch.from_numpy(np.array([frame_info['attributes'][0] for frame_info in frames])).long()\n",
        "\n",
        "        # 모델 예측\n",
        "        output = model(patches)\n",
        "\n",
        "        # 손실 계산 및 Optimizer 업데이트\n",
        "        loss = criterion(output, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 학습 진행 상황 출력\n",
        "        writer.add_scalar('Loss/train', loss, epoch)\n",
        "\n",
        "    # 검증 코드\n",
        "    # ...\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), 'model.ckpt')\n",
        "\n",
        "print('모델 학습 완료!')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "xWoLBMuC02PB",
        "outputId": "8d08946e-e409-4161-a5e9-5611e58031f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'timm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-38a527d0c627>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'timm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "\n",
        "# Function to parse annotation XML file and extract bounding box coordinates\n",
        "def parse_annotation(annotation_file):\n",
        "    tree = ET.parse(annotation_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract bounding box coordinates for each frame\n",
        "    bounding_boxes = []\n",
        "    for pedestrian in root.findall('.//pedestrian'):\n",
        "        for frame in pedestrian.findall('frame'):\n",
        "            bbox_attrib = frame.find('bbox').attrib\n",
        "            xtl = float(bbox_attrib['xtl'])\n",
        "            ytl = float(bbox_attrib['ytl'])\n",
        "            xbr = float(bbox_attrib['xbr'])\n",
        "            ybr = float(bbox_attrib['ybr'])\n",
        "            bounding_boxes.append((xtl, ytl, xbr, ybr))\n",
        "\n",
        "    return bounding_boxes\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# class STN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(STN, self).__init__()\n",
        "\n",
        "#         # Localization network (for predicting transformation parameters)\n",
        "#         self.localization = nn.Sequential(\n",
        "#             nn.Conv2d(3, 8, kernel_size=7),\n",
        "#             nn.MaxPool2d(2, stride=2),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(8, 10, kernel_size=5),\n",
        "#             nn.MaxPool2d(2, stride=2),\n",
        "#             nn.ReLU(True)\n",
        "#         )\n",
        "\n",
        "#         # Regressor for predicting transformation parameters\n",
        "#         self.fc_loc = nn.Sequential(\n",
        "#             nn.Linear(10 * 3 * 3, 32),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Linear(32, 3 * 2)  # Predicts 2x3 affine matrix\n",
        "#         )\n",
        "\n",
        "#         # Initialize the weights/bias with identity transformation\n",
        "#         self.fc_loc[-1].weight.data.fill_(0)\n",
        "#         self.fc_loc[-1].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
        "\n",
        "#     # Spatial transformer network forward function\n",
        "#     def stn(self, x):\n",
        "#         xs = self.localization(x)\n",
        "#         xs = xs.view(-1, 10 * 3 * 3)\n",
        "#         theta = self.fc_loc(xs)\n",
        "#         theta = theta.view(-1, 2, 3)\n",
        "\n",
        "#         # Grid generator and sampler\n",
        "#         grid = F.affine_grid(theta, x.size())\n",
        "#         x = F.grid_sample(x, grid)\n",
        "#         return x\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Apply STN to input\n",
        "#         x = self.stn(x)\n",
        "#         return x\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "class STN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(STN, self).__init__()\n",
        "\n",
        "        # Localization network (for predicting transformation parameters)\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Regressor for predicting transformation parameters\n",
        "        # self.fc_loc = nn.Sequential(\n",
        "        #     nn.Linear(10 * 3 * 3, 32),\n",
        "        #     nn.ReLU(True),\n",
        "        #     nn.Linear(32, 3 * 2)  # Predicts 2x3 affine matrix\n",
        "        # )\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(10 * ((224 - 6) // 4 - 4) * ((224 - 6) // 4 - 4), 32),  # Adjust input size\n",
        "            # nn.Linear(10 * 4 * 4, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)  # Predicts 2x3 affine matrix\n",
        "        )\n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc[-1].weight.data.fill_(0)\n",
        "        self.fc_loc[-1].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
        "\n",
        "    # Spatial transformer network forward function\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        # xs = xs.reshape(-1, 10 * 52 * 52)\n",
        "        xs = xs.reshape(-1, 10 * 4 * 4)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        # Grid generator and sampler\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "    # Method to extract pedestrian bounding box using annotation file\n",
        "    def extract_bbox(self, frame_number, annotation_file):\n",
        "        tree = ET.parse(annotation_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Find the frame with the given frame number\n",
        "        for pedestrian in root.findall('.//pedestrian'):\n",
        "            for frame in pedestrian.findall('frame'):\n",
        "                if int(frame.get('number')) == frame_number:\n",
        "                    bbox_attrib = frame.find('bbox').attrib\n",
        "                    xtl = float(bbox_attrib['xtl'])\n",
        "                    ytl = float(bbox_attrib['ytl'])\n",
        "                    xbr = float(bbox_attrib['xbr'])\n",
        "                    ybr = float(bbox_attrib['ybr'])\n",
        "                    return (xtl, ytl, xbr, ybr)\n",
        "\n",
        "        print(f\"Error: Frame {frame_number} not found in annotation file.\")\n",
        "        return None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply STN to input\n",
        "        x = self.stn(x)\n",
        "        return x\n",
        "\n",
        "def crop_bbox(frame, bbox):\n",
        "    \"\"\"\n",
        "    Crop the bounding box from the frame.\n",
        "\n",
        "    Args:\n",
        "    - frame (numpy.ndarray): Input frame (image) from which to crop the bounding box.\n",
        "    - bbox (tuple): Tuple containing the bounding box coordinates (xtl, ytl, xbr, ybr).\n",
        "\n",
        "    Returns:\n",
        "    - cropped_bbox (numpy.ndarray): Cropped bounding box from the frame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract bounding box coordinates\n",
        "    xtl, ytl, xbr, ybr = bbox\n",
        "\n",
        "    # Ensure bounding box coordinates are within frame boundaries\n",
        "    xtl = max(0, int(xtl))\n",
        "    ytl = max(0, int(ytl))\n",
        "    xbr = min(frame.shape[1], int(xbr))\n",
        "    ybr = min(frame.shape[0], int(ybr))\n",
        "\n",
        "    # Crop the bounding box from the frame\n",
        "    cropped_bbox = frame[ytl:ybr, xtl:xbr]\n",
        "\n",
        "    return cropped_bbox\n",
        "# Function to apply STN to extract pedestrian bounding box from each frame\n",
        "def apply_stn(video_path, annotation_path):\n",
        "    # Load video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Couldn't open video file\")\n",
        "        return\n",
        "\n",
        "    # Parse annotation file\n",
        "    annotation_file = os.path.join(annotation_path, os.path.basename(video_path).split('.')[0] + '.xml')\n",
        "    bounding_boxes = parse_annotation(annotation_file)\n",
        "\n",
        "    # Initialize STN model and ViViT model (you need to define these)\n",
        "    stn_model = STN()\n",
        "    # vivit_model = YourViViTModel()\n",
        "    frame_number = 0\n",
        "    # Process each frame\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Apply STN to extract pedestrian bounding box\n",
        "        # Here, we assume stn_model.extract_bbox is the method to extract the bounding box\n",
        "        pedestrian_bbox = stn_model.extract_bbox(frame_number, annotation_file)\n",
        "        if pedestrian_bbox is None:\n",
        "            print(f\"No annotation found for frame {frame_number}\")\n",
        "            frame_number += 1\n",
        "            continue\n",
        "        # Pass the extracted pedestrian bounding box to ViViT model for action estimation\n",
        "        # You need to define how to resize/crop the bounding box and pass it to ViViT model\n",
        "        # Here, we assume crop_bbox is the function to crop the bounding box from the frame\n",
        "        pedestrian_crop = crop_bbox(frame, pedestrian_bbox)\n",
        "        pedestrian_crop = cv2.resize(pedestrian_crop, (224, 224))  # Assuming ViViT input size\n",
        "        pedestrian_crop = np.expand_dims(pedestrian_crop, axis=0)  # Adding batch dimension\n",
        "\n",
        "        # Forward pass through ViViT model to estimate action\n",
        "        # action_prediction = vivit_model.predict(pedestrian_crop)\n",
        "        # print(\"Action Prediction:\", action_prediction)  # Modify based on your output format\n",
        "\n",
        "        frame_number+=1\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "video_path = \"videos/video_0001.mp4\"\n",
        "annotation_path = \"/content/vivit_labels/\"\n",
        "apply_stn(video_path, annotation_path)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_stn_random_frame(video_path, annotation_path, stn_model):\n",
        "    # Load video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Couldn't open video file\")\n",
        "        return\n",
        "\n",
        "    # Parse annotation file\n",
        "    annotation_file = os.path.join(annotation_path, os.path.basename(video_path).split('.')[0] + '.xml')\n",
        "    bounding_boxes = parse_annotation(annotation_file)\n",
        "\n",
        "    # Select a random frame\n",
        "    frame_number = np.random.randint(0, 300)\n",
        "\n",
        "    # Extract the bounding box for the selected frame\n",
        "    pedestrian_bbox = bounding_boxes[frame_number]\n",
        "\n",
        "    # Read the selected frame\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Couldn't read frame\")\n",
        "        return\n",
        "\n",
        "    # Apply STN to extract pedestrian bounding box\n",
        "    pedestrian_crop = crop_bbox(frame, pedestrian_bbox)\n",
        "    pedestrian_crop = cv2.resize(pedestrian_crop, (224, 224))  # Assuming ViViT input size\n",
        "    pedestrian_crop = np.expand_dims(pedestrian_crop, axis=0)  # Adding batch dimension\n",
        "\n",
        "    # Forward pass through STN model\n",
        "    output = stn_model(torch.tensor(pedestrian_crop).permute(0, 3, 1, 2).float())\n",
        "    print(\"STN output shape:\", output.shape)\n",
        "\n",
        "    # Visualize original frame and cropped bounding box\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Original Frame\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cv2.cvtColor(pedestrian_crop[0], cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Cropped Pedestrian Bounding Box\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    cap.release()\n",
        "stn_model = STN()\n",
        "test_stn_random_frame(video_path, annotation_path, stn_model)"
      ],
      "metadata": {
        "id": "3wivJedRouu0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "dd8e94b8-fc31-4765-8ce1-0a12a4d445f1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (169x160 and 25000x32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-128015e122e1>\u001b[0m in \u001b[0;36m<cell line: 265>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m \u001b[0mtest_stn_random_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-128015e122e1>\u001b[0m in \u001b[0;36mtest_stn_random_frame\u001b[0;34m(video_path, annotation_path, stn_model)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# Forward pass through STN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpedestrian_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STN output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-501d4a06dd6f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# Apply STN to input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-501d4a06dd6f>\u001b[0m in \u001b[0;36mstn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# xs = xs.reshape(-1, 10 * 52 * 52)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (169x160 and 25000x32)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOl4PSZTaLk/S5Hk679xToe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}